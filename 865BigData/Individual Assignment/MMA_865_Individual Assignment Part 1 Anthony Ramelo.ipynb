{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMA 865, Individual Assignment 1\n",
    "\n",
    "Last Updated December 11, 2023.\n",
    "\n",
    "- Anthony Ramelo\n",
    "- 20499391\n",
    "- Jan 21, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Sentiment Analysis via the ML-based approach\n",
    "\n",
    "Download the “Product Sentiment” dataset from the course portal: sentiment_train.csv and sentiment_test.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.a. Loading and Prep\n",
    "\n",
    "Load, clean, and preprocess the data as you find necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anthonyramelo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/anthonyramelo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spellchecker import SpellChecker\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize resources\n",
    "default_stop_words = set(stopwords.words('english'))\n",
    "custom_stop_words = default_stop_words - {\"not\", \"no\", \"never\"}\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Load datasets\n",
    "df_train = pd.read_csv(\"sentiment_train.csv\")\n",
    "df_test = pd.read_csv(\"sentiment_test.csv\")\n",
    "\n",
    "# Ensure 'Sentence' column exists and remove null/empty rows\n",
    "df_train.dropna(subset=['Sentence'], inplace=True)\n",
    "df_test.dropna(subset=['Sentence'], inplace=True)\n",
    "\n",
    "# Updated text cleaning function to retain double negatives\n",
    "def clean_text_v4(text):\n",
    "    # Remove placeholders, URLs, special characters, and numbers\n",
    "    text = re.sub(r'#NAME\\?|http\\S+|www\\S+|https\\S+|\\@\\w+|\\#|\\d+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuations\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    # Tokenize the text and remove stop words while retaining 'not', 'no', and 'never'\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in custom_stop_words]\n",
    "    # Perform spell check\n",
    "    corrected_words = [spell.correction(word) if spell.correction(word) else word for word in filtered_words]\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "# Apply updated cleaning function to datasets\n",
    "df_train['Cleaned_Sentence'] = df_train['Sentence'].apply(clean_text_v4)\n",
    "df_test['Cleaned_Sentence'] = df_test['Sentence'].apply(clean_text_v4)\n",
    "df_train.dropna(subset=['Cleaned_Sentence'], inplace=True)\n",
    "df_test.dropna(subset=['Cleaned_Sentence'], inplace=True)\n",
    "\n",
    "# Features and target\n",
    "X_train = df_train['Cleaned_Sentence']\n",
    "y_train = df_train['Polarity']\n",
    "X_test = df_test['Cleaned_Sentence']\n",
    "y_test = df_test['Polarity']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.b. Modeling\n",
    "\n",
    "Use your favorite ML algorithm to train a classification model.  Don’t forget everything that we’ve learned in our ML course: hyperparameter tuning, cross validation, handling imbalanced data, etc. Make reasonable decisions and try to create the best-performing classifier that you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Metrics:\n",
      "Accuracy: 0.7233\n",
      "F1 Score: 0.6914\n",
      "ROC-AUC: 0.8127\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 1213, number of negative: 1213\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003917 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1691\n",
      "[LightGBM] [Info] Number of data points in the train set: 2426, number of used features: 103\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "LightGBM Metrics:\n",
      "Accuracy: 0.6383\n",
      "F1 Score: 0.6004\n",
      "ROC-AUC: 0.7314\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 1213, number of negative: 1213\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1691\n",
      "[LightGBM] [Info] Number of data points in the train set: 2426, number of used features: 103\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 970, number of negative: 971\n",
      "[LightGBM] [Info] Number of positive: 970, number of negative: 970\n",
      "[LightGBM] [Info] Number of positive: 971, number of negative: 970\n",
      "[LightGBM] [Info] Number of positive: 971, number of negative: 970\n",
      "[LightGBM] [Info] Number of positive: 970, number of negative: 971\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007236 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1180\n",
      "[LightGBM] [Info] Number of data points in the train set: 1941, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013729 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1219\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014237 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of data points in the train set: 1941, number of used features: 75\n",
      "[LightGBM] [Info] Total Bins 1208\n",
      "[LightGBM] [Info] Number of data points in the train set: 1940, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1211\n",
      "[LightGBM] [Info] Number of data points in the train set: 1941, number of used features: 77\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1195\n",
      "[LightGBM] [Info] Number of data points in the train set: 1941, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Stacked Model Metrics:\n",
      "Accuracy: 0.7200\n",
      "F1 Score: 0.6900\n",
      "ROC-AUC: 0.8096\n",
      "\n",
      "Logistic Regression Metrics:\n",
      "Accuracy: 0.7733\n",
      "F1 Score: 0.7631\n",
      "ROC-AUC: 0.8573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# SMOTE for balancing\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# Model 1: XGBoost\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred_xgb = xgb_model.predict(X_test_tfidf)\n",
    "y_proba_xgb = xgb_model.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "print(\"XGBoost Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba_xgb):.4f}\\n\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# Model 2: LightGBM\n",
    "lgbm_model = LGBMClassifier(random_state=42, class_weight='balanced')\n",
    "lgbm_model.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred_lgbm = lgbm_model.predict(X_test_tfidf)\n",
    "y_proba_lgbm = lgbm_model.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "print(\"LightGBM Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lgbm):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_lgbm):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba_lgbm):.4f}\\n\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# Model 3: Stacked (XGBoost + LightGBM)\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=[('xgb', xgb_model), ('lgbm', lgbm_model)],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "stacked_model.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred_stacked = stacked_model.predict(X_test_tfidf)\n",
    "y_proba_stacked = stacked_model.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "print(\"Stacked Model Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_stacked):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_stacked):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba_stacked):.4f}\\n\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# Model 4: Logistic Regression\n",
    "lr_model = LogisticRegression(solver='liblinear', random_state=42)\n",
    "lr_model.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred_lr = lr_model.predict(X_test_tfidf)\n",
    "y_proba_lr = lr_model.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "print(\"Logistic Regression Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba_lr):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.c. Assessing\n",
    "\n",
    "Use the testing data to measure the accuracy and F1-score of your model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics:\n",
      "Accuracy: 0.7733\n",
      "F1 Score: 0.7631\n",
      "ROC-AUC: 0.8573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba_lr):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Given the accuracy and F1-score of your model, are you satisfied with the results, from a business point of view? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a business point of view, the model’s performance is not satisfactory, particularly for scenarios where there is a high cost if the decision of the output is wrong. Although the model achieves an accuracy of 77.33%, this means that around 23% of predictions are incorrect, which is a significant error rate. This inaccuracy could lead to poor decision-making, especially in situations where the sentiment analysis is critical to business success.\n",
    "\n",
    "The F1-score of 76.31% shows that the model balances precision and recall but has trouble with harder cases and less common sentence structures. This is a concern if accurately identifying nuanced or ambiguous sentiments is important, as the model’s performance may not consistently meet business needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Show five example instances in which your model’s predictions were incorrect. Describe why you think the model was wrong. Don’t just guess: dig deep to figure out the root cause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Cleaned_Sentence</th>\n",
       "      <th>Predicted_Polarity</th>\n",
       "      <th>Probability</th>\n",
       "      <th>True_Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>But I thought his acting was skilled.</td>\n",
       "      <td>1</td>\n",
       "      <td>thought acting skilled</td>\n",
       "      <td>0</td>\n",
       "      <td>0.473471</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>But it is entertaining, nonetheless.</td>\n",
       "      <td>1</td>\n",
       "      <td>entertaining nonetheless</td>\n",
       "      <td>0</td>\n",
       "      <td>0.471712</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>You wont regret it!</td>\n",
       "      <td>1</td>\n",
       "      <td>wont regret</td>\n",
       "      <td>0</td>\n",
       "      <td>0.463258</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>Omit watching this.</td>\n",
       "      <td>0</td>\n",
       "      <td>omit watching</td>\n",
       "      <td>1</td>\n",
       "      <td>0.504836</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>If you act in such a film, you should be glad ...</td>\n",
       "      <td>0</td>\n",
       "      <td>act film glad your gonna drift away earth far ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.584955</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  Polarity  \\\n",
       "351            But I thought his acting was skilled.           1   \n",
       "240             But it is entertaining, nonetheless.           1   \n",
       "294                              You wont regret it!           1   \n",
       "214                              Omit watching this.           0   \n",
       "572  If you act in such a film, you should be glad ...         0   \n",
       "\n",
       "                                      Cleaned_Sentence  Predicted_Polarity  \\\n",
       "351                             thought acting skilled                   0   \n",
       "240                           entertaining nonetheless                   0   \n",
       "294                                        wont regret                   0   \n",
       "214                                      omit watching                   1   \n",
       "572  act film glad your gonna drift away earth far ...                   1   \n",
       "\n",
       "     Probability  True_Polarity  \n",
       "351     0.473471              1  \n",
       "240     0.471712              1  \n",
       "294     0.463258              1  \n",
       "214     0.504836              0  \n",
       "572     0.584955              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Identify incorrect predictions\n",
    "incorrect_indices = (y_test != y_pred_lr)\n",
    "\n",
    "# Extract incorrect predictions\n",
    "incorrect_examples = df_test.loc[incorrect_indices].copy()\n",
    "incorrect_examples['Predicted_Polarity'] = y_pred_lr[incorrect_indices]\n",
    "incorrect_examples['Probability'] = y_proba_lr[incorrect_indices]\n",
    "\n",
    "# Select 5 random incorrect examples for detailed analysis\n",
    "sample_incorrect = incorrect_examples.sample(5, random_state=42)\n",
    "\n",
    "# Add true labels to the sample\n",
    "sample_incorrect['True_Polarity'] = y_test.loc[sample_incorrect.index].values\n",
    "\n",
    "# Add cleaned sentences for context\n",
    "sample_incorrect['Cleaned_Sentence'] = sample_incorrect['Sentence'].apply(clean_text_v4)\n",
    "sample_incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model made mistakes because it struggles with certain types of sentences. For example, in “But I thought his acting was skilled,” the word “but” made the model think the sentence was negative, even though it was positive. In “But it is entertaining, nonetheless,” the word “nonetheless” may have confused it. It also had trouble with negatives, like in “You won’t regret it!”, where it focused on “regret” but didn’t recognize the “won’t.” The model also misunderstood commands, like in “Omit watching this,” where it saw “watching” as positive and ignored “omit.” Finally, it didn’t pick up on sarcasm in “If you act in such a film, you should be glad…”, treating it as a positive statement. These mistakes show that the model needs to get better at understanding negatives, context, and sarcasm to make more accurate predictions.\n",
    "\n",
    "Analysis on each of the sentences:\n",
    "\n",
    "1. “But I thought his acting was skilled.”\n",
    "\n",
    "\t•\tWords like “but” might dilute the sentiment. Additionally, “skilled” is a positive word, but the phrase “I thought” might weaken its overall weight.\n",
    "\n",
    "2. “But it is entertaining, nonetheless.”\n",
    "\n",
    "\t•\tThe word “entertaining” is positive, but “nonetheless” makes the sentence complex. The \"But\" may have also have some weight to the sentence.\n",
    "\n",
    "3. “You won’t regret it!”\n",
    "\n",
    "\t•\tThe model did not handle the double negative of \"won't\" and \"regret\" well.\n",
    "\n",
    "4. “Omit watching this.”\n",
    "\t•\tThe word \"watching\" being positive and \"Omit\" being negative. Did have an affect on the classification of the sentence.\n",
    "\n",
    "5. “If you act in such a film, you should be glad your gonna drift away earth far …”\n",
    "\t•\tThe phrase “you should be glad” might have confused the model since this sentence is sarcasm. \n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
