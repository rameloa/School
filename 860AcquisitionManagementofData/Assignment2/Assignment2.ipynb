{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tYour manager doesn’t know much about analytics.  That is too bad, but don’t worry, you’ll have his job soon enough.  In the meantime he has made the following comments.  If his suggestions / comments are good ones, explain why; if they are bad ones, explain why and what you should do to fix them or do better.  Make sure your answer demonstrates that you have a sophisticated understanding of the issues involved. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a.\t“If the p-value for the ‘joint F-test’ is too large – say greater than 0.05 – there is really no point in looking at the rest of the regression output.  The model is just plain crap.” 3 marks\n",
    "\n",
    "** A large F-Value suggests that we should reject the null hypothesis. Meaning that some of variables in the model may not be significant for predicting. This does not mean that we should dismiss all variables in the model, but instead look at each individual variable to determine the significance to the predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1b.\t“When you detect an outlier, you should just replace it with the mean value of the dataset.” 3 marks\n",
    "\n",
    "** Outliers can significantly influence the mean of a dataset, making it a biased replacement value. Moreover, simply replacing outliers with the mean does not address the underlying reasons for their existence and can lead to misleading analysis. Outliers should be carefully analyzed to understand why they occurred—whether they result from data entry errors, measurement errors, or represent a natural but rare event in the distribution. **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1c.\t“When conducting a hypothesis test, like the t-test, you should always set alpha = 0.05.” 3 marks\n",
    "\n",
    "** The choice of alpha (significance level) should be context-specific and not just set by convention. Alpha = 0.05 is commonly used but choosing it should depend on the balance between Type I errors (false positives) and Type II errors (false negatives), as well as the consequences of each in the specific context. In scenarios where the cost of a Type I error is high, a smaller alpha might be appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1d.\t“Our hotel did a customer satisfaction survey for all the guests who visited last weekend.  The results were plagued by heteroskedasticity, so we basically had to scrap the results and start again.” 3 marks\n",
    "\n",
    "** Imagine you’re taking photos with a camera where the focus isn’t consistent. Some photos come out crisp (low variance), and some are blurry (high variance), depending on how much light there is. Heteroskedasticity in statistics is similar: the spread or “noise” in your data changes depending on the value of what you’re measuring. This can mess up your analysis because the usual statistical tests assume the noise level is consistent across all measurements.\n",
    "\n",
    "However, just like you wouldn’t throw out all photos because some are blurry, you don’t need to discard your survey or study results if there’s heteroskedasticity. You can adjust your methods to handle this inconsistency.\n",
    "\n",
    "Simple Recommendation\n",
    "\n",
    "Instead of tossing out your data when you find varying noise levels (heteroskedasticity), you can use special tools and techniques to correct for it. These include:\n",
    "\n",
    "1.\tChanging the scale of the data: Like switching from measuring in square feet to logarithm of square feet to even out the noise.\n",
    "2.\tWeighted methods: Giving more importance to more reliable (less noisy) data when you analyze it, sort of like paying more attention to the clearer photos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\tYour client has asked you to perform some analysis on the data found on the tab Missing.\n",
    "a.\tExplain in language that your manager is likely to understand how multiple imputation deals with missing values. 2 marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Obs     100 non-null    int64  \n",
      " 1   Y       100 non-null    int64  \n",
      " 2   X1      99 non-null     float64\n",
      " 3   X2      95 non-null     float64\n",
      " 4   X3      99 non-null     float64\n",
      " 5   X4      96 non-null     float64\n",
      " 6   X5      98 non-null     float64\n",
      "dtypes: float64(5), int64(2)\n",
      "memory usage: 5.6 KB\n"
     ]
    }
   ],
   "source": [
    "#Importing Data\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "db_dir = os.getcwd()\n",
    "df = pd.read_excel(db_dir + r'/MMA860 Assignment 2 Data.xlsx', sheet_name='Missing')\n",
    "df.info()\n",
    "\n",
    "\n",
    "# Imputation is a process used to substitute missing data with estimated values. Although this initially seems beneficial, the estimated values often have \n",
    "# less variability compared to the original data, which can lead to inaccuracies. To mitigate this, multiple imputation techniques are commonly used. \n",
    "# In multiple imputation, several imputed datasets are created where the missing values are filled using a model that incorporates both an estimate \n",
    "# of the missing value and a term for random error to introduce variability. This model is applied multiple times, and the results are averaged to \n",
    "# derive a final estimate. This process helps reduce the risk of overfitting the data, which can occur when a model is too closely tailored to a \n",
    "# specific set of data points.\n",
    "\n",
    "# Software Automation and Variability:\n",
    "# The entire process of multiple imputation, including the generation of multiple datasets and the averaging of outcomes, is typically automated \n",
    "# within statistical software. This makes the process efficient and user-friendly, as users do not need to perform these calculations manually.\n",
    "\n",
    "# Addressing Different Types of Missing Data:\n",
    "# The approach can be adjusted to handle different patterns of missing data, such as Missing Completely at Random (MCAR) or Missing at Random (MAR). \n",
    "# In larger samples, these adjustments are more reliable and allow for more accurate imputation.\n",
    "\n",
    "# This explanation demonstrates the practical applications and benefits of multiple imputation in statistical analysis, particularly in handling \n",
    "# missing data while maintaining the integrity and reliability of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b.\tUnder what condition(s) could multiple imputation be used reliably to deal with missing values.  Provide an original example (i.e. not ones that I have provided or that we have discussed in class) to illustrate when multiple imputation could reliably be used – if there are more than one condition, be sure to illustrate them all and describe how they apply in your example. 4 marks\n",
    "\n",
    "** Multiple imputation can be reliably used to handle missing data under certain conditions, primarily revolving around the nature and pattern of the missing data. Here are the key conditions and an illustrative example:\n",
    "\n",
    "Conditions for Reliable Use of Multiple Imputation\n",
    "\n",
    "1.\tMissing at Random (MAR): Multiple imputation works best when the missingness of the data is not related to the missing values themselves but may be related to other observed data. This means that the propensity for a data point to be missing is related to some other measured variable in the dataset, but not to the value of the missing data itself.\n",
    "2.\tSufficient Sample Size: There should be enough complete cases or partially complete cases with sufficient overlapping information that can help in predicting the missing values accurately.\n",
    "3.\tAppropriate Model for Imputation: The statistical models used to impute values must be correctly specified, including the relationships between variables. A poor model can lead to biased imputation.\n",
    "\n",
    "Scenario: A company wants to analyze the factors affecting employee productivity. The dataset includes variables such as hours worked, job satisfaction, stress levels, and productivity scores. However, some values are missing across these variables.\n",
    "\n",
    "1. Hours Worked: Recorded via an automated system; no missing data.\\\n",
    "2. Job Satisfaction: Survey-based; missing data as not all employees completed the survey.\n",
    "3. Stress Levels: Also survey-based; missing similarly to job satisfaction.\n",
    "4. Productivity Scores: Assessed by supervisors; missing in cases where supervisors failed to submit evaluations.\n",
    "\n",
    "Application of Conditions:\n",
    "\n",
    "1. MAR: We assume that the missingness in job satisfaction and stress levels is related to observable variables like department or role, which might influence survey participation (e.g., busier departments might have lower response rates). However, it’s not directly related to the employees’ actual satisfaction or stress levels. Similarly, missing productivity scores could be related to the supervisor’s workload and not to the employee’s actual productivity.\n",
    "2. Sufficient Sample Size: The company has a large number of employees across various departments, providing enough data points with varying degrees of overlap between the observed variables. This overlap can help in reliably predicting the missing values.\n",
    "3. Appropriate Model for Imputation: The imputation model could include predictors such as department, role, and hours worked to estimate job satisfaction and stress levels. For productivity scores, predictors might include job satisfaction, stress levels, and hours worked.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c.\tIf the conditions you describe above were not met, what else could you do? What might some problems / concerns be with such an approach? 2 marks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d.\tEstimate the model y = B0 + B1 X1 + B2 X2 + … B5 X5 using multiple imputation to correct for missing values.  2 marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>Y</td>        <th>  R-squared:         </th> <td>   0.236</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.189</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   5.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 15 Jul 2024</td> <th>  Prob (F-statistic):</th> <td>0.000476</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:04:55</td>     <th>  Log-Likelihood:    </th> <td> -723.02</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    87</td>      <th>  AIC:               </th> <td>   1458.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    81</td>      <th>  BIC:               </th> <td>   1473.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td> 1101.8572</td> <td>  567.951</td> <td>    1.940</td> <td> 0.056</td> <td>  -28.186</td> <td> 2231.901</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X1</th>        <td>    6.4989</td> <td>    3.816</td> <td>    1.703</td> <td> 0.092</td> <td>   -1.094</td> <td>   14.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X2</th>        <td>   13.8281</td> <td>    7.820</td> <td>    1.768</td> <td> 0.081</td> <td>   -1.732</td> <td>   29.388</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X3</th>        <td>   14.7831</td> <td>    4.836</td> <td>    3.057</td> <td> 0.003</td> <td>    5.161</td> <td>   24.405</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X4</th>        <td>   11.0764</td> <td>    5.988</td> <td>    1.850</td> <td> 0.068</td> <td>   -0.837</td> <td>   22.990</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X5</th>        <td>    2.2195</td> <td>    4.863</td> <td>    0.456</td> <td> 0.649</td> <td>   -7.457</td> <td>   11.896</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 4.858</td> <th>  Durbin-Watson:     </th> <td>   1.616</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.088</td> <th>  Jarque-Bera (JB):  </th> <td>   4.315</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.536</td> <th>  Prob(JB):          </th> <td>   0.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.198</td> <th>  Cond. No.          </th> <td>    622.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        Y         & \\textbf{  R-squared:         } &     0.236   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.189   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     5.010   \\\\\n",
       "\\textbf{Date:}             & Mon, 15 Jul 2024 & \\textbf{  Prob (F-statistic):} &  0.000476   \\\\\n",
       "\\textbf{Time:}             &     21:04:55     & \\textbf{  Log-Likelihood:    } &   -723.02   \\\\\n",
       "\\textbf{No. Observations:} &          87      & \\textbf{  AIC:               } &     1458.   \\\\\n",
       "\\textbf{Df Residuals:}     &          81      & \\textbf{  BIC:               } &     1473.   \\\\\n",
       "\\textbf{Df Model:}         &           5      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &    1101.8572  &      567.951     &     1.940  &         0.056        &      -28.186    &     2231.901     \\\\\n",
       "\\textbf{X1}        &       6.4989  &        3.816     &     1.703  &         0.092        &       -1.094    &       14.092     \\\\\n",
       "\\textbf{X2}        &      13.8281  &        7.820     &     1.768  &         0.081        &       -1.732    &       29.388     \\\\\n",
       "\\textbf{X3}        &      14.7831  &        4.836     &     3.057  &         0.003        &        5.161    &       24.405     \\\\\n",
       "\\textbf{X4}        &      11.0764  &        5.988     &     1.850  &         0.068        &       -0.837    &       22.990     \\\\\n",
       "\\textbf{X5}        &       2.2195  &        4.863     &     0.456  &         0.649        &       -7.457    &       11.896     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  4.858 & \\textbf{  Durbin-Watson:     } &    1.616  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.088 & \\textbf{  Jarque-Bera (JB):  } &    4.315  \\\\\n",
       "\\textbf{Skew:}          & -0.536 & \\textbf{  Prob(JB):          } &    0.116  \\\\\n",
       "\\textbf{Kurtosis:}      &  3.198 & \\textbf{  Cond. No.          } &     622.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      Y   R-squared:                       0.236\n",
       "Model:                            OLS   Adj. R-squared:                  0.189\n",
       "Method:                 Least Squares   F-statistic:                     5.010\n",
       "Date:                Mon, 15 Jul 2024   Prob (F-statistic):           0.000476\n",
       "Time:                        21:04:55   Log-Likelihood:                -723.02\n",
       "No. Observations:                  87   AIC:                             1458.\n",
       "Df Residuals:                      81   BIC:                             1473.\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept   1101.8572    567.951      1.940      0.056     -28.186    2231.901\n",
       "X1             6.4989      3.816      1.703      0.092      -1.094      14.092\n",
       "X2            13.8281      7.820      1.768      0.081      -1.732      29.388\n",
       "X3            14.7831      4.836      3.057      0.003       5.161      24.405\n",
       "X4            11.0764      5.988      1.850      0.068      -0.837      22.990\n",
       "X5             2.2195      4.863      0.456      0.649      -7.457      11.896\n",
       "==============================================================================\n",
       "Omnibus:                        4.858   Durbin-Watson:                   1.616\n",
       "Prob(Omnibus):                  0.088   Jarque-Bera (JB):                4.315\n",
       "Skew:                          -0.536   Prob(JB):                        0.116\n",
       "Kurtosis:                       3.198   Cond. No.                         622.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frmla = 'Y ~ X1+X2+X3+X4+X5'\n",
    "    \n",
    "from statsmodels.formula.api import ols\n",
    "results = ols(frmla,df).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Results: MICE\n",
      "===================================================================\n",
      "Method:                   MICE      Sample size:          100      \n",
      "Model:                    OLS       Scale                 968572.53\n",
      "Dependent variable:       Y         Num. imputations      5        \n",
      "-------------------------------------------------------------------\n",
      "            Coef.   Std.Err.   t    P>|t|   [0.025   0.975]   FMI  \n",
      "-------------------------------------------------------------------\n",
      "Intercept 1002.8614 507.8499 1.9747 0.0483  7.4939 1998.2289 0.0170\n",
      "X1           6.1726   3.5088 1.7592 0.0785 -0.7045   13.0496 0.0186\n",
      "X2          15.0389   7.0687 2.1275 0.0334  1.1845   28.8933 0.0145\n",
      "X3          15.3872   4.4694 3.4428 0.0006  6.6274   24.1470 0.0096\n",
      "X4          10.6157   5.5061 1.9280 0.0539 -0.1761   21.4076 0.0334\n",
      "X5           3.6170   4.3859 0.8247 0.4095 -4.9792   12.2133 0.0100\n",
      "===================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.imputation.mice as mice\n",
    "import statsmodels.regression.linear_model as sm\n",
    "\n",
    "#Impute the data set with MICE Data\n",
    "imp = mice.MICEData(df)\n",
    "imputed_data = imp.next_sample()\n",
    "merge_data = pd.merge(imputed_data,df,left_index=True,right_index=True)\n",
    "merge_data.columns\n",
    "merge_data = merge_data[['Obs_x', 'Y_x', 'X1_x', 'X2_x', 'X3_x', 'X4_x', 'X5_x']]\n",
    "\n",
    "merge_data[(pd.isnull(merge_data))]\n",
    "mice = mice.MICE(frmla, sm.OLS, imp)\n",
    "results = mice.fit(n_imputations=5)\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** e.\tAccording to your results, does X2 belong in that model?  Explain why / why not 2 marks \n",
    "\n",
    "Before imputation,  X2  did not seem to belong in the model due to a lack of statistical significance. However, after imputation, the statistical significance improved, suggesting a potentially important role for  X2  in the model. This highlights the importance of handling missing data appropriately to avoid misleading statistical inferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\t I am interested in collecting wine, and typically buy Italian wine with a cork (because no good wine can come with a screw top, right?). I rely very heavily on expert ratings to determine which wines I buy. However, whenever a wine with a high rating is released, it immediately sells out. Despite many complaints, the LCBO does not want to help. Using the data set provided on historical wine ratings and their characteristics, answer the following questions with a model that predicts expert ratings. Note: you should read all parts of the question before answering, and build a single model to answer all 4 parts.\n",
    "a.\tPresent your final model and the estimated parameters. What steps did you go through to develop this model? 2 marks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-Squared: 0.8230004928894843\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m reg \u001b[38;5;241m=\u001b[39m LinearRegression()\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR-Squared:\u001b[39m\u001b[38;5;124m\"\u001b[39m, reg\u001b[38;5;241m.\u001b[39mscore(X_train, y_train))\n\u001b[0;32m---> 28\u001b[0m model \u001b[38;5;241m=\u001b[39m ols(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRating ~ Price + Alcohol + Sulphates + CountryFrance\u001b[39m\u001b[38;5;124m'\u001b[39m,data)\u001b[38;5;241m.\u001b[39mfit()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39msummary())\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#Testing for heteroskasticity \u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#Residuals calculated by definition above.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ols' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_excel(db_dir + r'/MMA860 Assignment 2 Data.xlsx', sheet_name='Wine')\n",
    "\n",
    "# Define features and target variable\n",
    "import numpy as np\n",
    "data['CountryFrance'] = np.where(data['Country'] == 'France', 1, 0)\n",
    "\n",
    "X = data[['Price', 'Alcohol', 'Residual_Sugar', 'Sulphates', 'pH','CountryFrance']].values\n",
    "y = data['Rating'].values\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.95,random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Fitting data to a regression model requires two arguments, the training X\n",
    "values (independent variables) and the training y values (dependent variables.\n",
    "In general, most fit functions for models follow this format.\n",
    "'''\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "print(\"R-Squared:\", reg.score(X_train, y_train))\n",
    "\n",
    "model = ols('Rating ~ Price + Alcohol + Sulphates + CountryFrance',data).fit()\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "#Testing for heteroskasticity \n",
    "\n",
    "#Residuals calculated by definition above.\n",
    "predicted_y = reg.predict(X_train)\n",
    "#Note we can perform element-wise subtraction between arrays like so\n",
    "residuals = y_train - predicted_y\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(predicted_y,residuals,s=2,c='black')\n",
    "\n",
    "#This line adds the dashed horizontal line\n",
    "plt.hlines(0,min(predicted_y),max(predicted_y),color='red',linestyles='dashed')\n",
    "\n",
    "plt.xlabel(\"Model Prediction\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.show()\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "#Import the relevant libraries and train the model\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "results = ols('Rating ~ Price + Alcohol  + Sulphates + pH',data).fit()\n",
    "\n",
    "#Perform the Breuch-Pagan Test by running this line\n",
    "bp = het_breuschpagan(results.resid,results.model.exog)\n",
    "measures = ('LM Statistic', 'LM-Test p-value', 'F-Statistic', 'F-Test p-value')\n",
    "print(dict(zip(measures,bp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Checked if the data had any missing values:\n",
    "No missing values were found in the dataset so MICE (Multiple Imputation by Chained Equations) was not needed to see if the missing values had significance to the model.\n",
    "2. Removed features that had a high P value:\n",
    "'Residual_Sugar' and 'pH'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3b.\tDoes the data appear to be heteroskedastic? Why or why not? Show evidence. 2 marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Check the data features for heteroskaticity:\n",
    "From the graph above, the data is scattered/ does not have a funnel shape meaning that it does not have heteroskaticity\n",
    "3. Perform a Breusch-Pagan test:\n",
    "The test results indicate that the null hypothesis of homoscedasticity (constant variance of the residuals) cannot be rejected because the p-values are well above the 0.05 threshold:\n",
    "    1. LM-Test p-value: 0.8627\n",
    "    2. F-Test p-value: 0.8718 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c.\t Assuming there are no data problems, what would a wine be rated if it comes from France, has a price of $39.99, sulphates of 1.1, alcohol of 13.9%, residual sugar of 1.83 and a pH of 2.1? 1 mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    72.802692\n",
       "1    78.045035\n",
       "dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.DataFrame({\n",
    "    'Price': [39.99,41.00],\n",
    "    'Alcohol': [13.9,13.9],\n",
    "    'Residual_Sugar': [1.83,0],\n",
    "    'Sulphates': [1.1,1.1],\n",
    "    'pH': [2.1,0],\n",
    "    'Country_France': [1,1],\n",
    "})\n",
    "\n",
    "# Add a constant for prediction with statsmodels\n",
    "features_sm = sm.add_constant(features)\n",
    "\n",
    "\n",
    "#spl\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.95,random_state=0)\n",
    "\n",
    "# Prediction using the built model\n",
    "predicted_rating = results.predict(features_sm)\n",
    "predicted_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d.\tWould increasing the price of a wine increase its expert rating? Be sure to clearly explain your thinking. 3 marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When playing around with the model to find the correlation between price and rating there looks to be a correlation with the increase in price and higher the rating... But since we are only looking at France, it would be best to look at other regions as well. This could help in determine if the model could be applied in other regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tYou might not know this about me, but I am pretty into ‘epic cooking’. I have, at times, cooked over 35 liters of curry in one batch (some for eating and some for preserving). I like to believe my curry is pretty good. In fact, I believe it is so good I may be able to sell it. I have simulated some data based on my understanding of the North American curry market in relation to price, advertising budget, how far away people live from the nearest retailer that might stock my curry, and what country people reside in. Of particular interest to me is whether Canadians and Americans feel differently about curry. \n",
    "    1. Build a model to predict sales. Include your model results and explain your process. 2 marks\n",
    "    2. I believe that US consumers respond more to changes in Ad_Budget than do Canadians.  Conduct the most powerful test you can to see if this is the case.  Briefly explain the test and your results (i.e. state the null, alternative, the p-value of the result and what it means.) 2 marks\n",
    "    3. Now conduct a Chow test to determine if Canadians are different from Americans.  Briefly explain these results, particularly in light of your test above. 3 marks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Obs', 'Sales', 'Ad_Budget', 'Price', 'Distance', 'Country'], dtype='object')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_excel(db_dir + r'/MMA860 Assignment 2 Data.xlsx', sheet_name='Curry')\n",
    "data.columns\n",
    "# ['Obs', 'Sales', 'Ad_Budget', 'Price', 'Distance', 'Country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define features and target variable\n",
    "import numpy as np\n",
    "data['CountryFrance'] = np.where(data['Country'] == 'France', 1, 0)\n",
    "\n",
    "train_X = data[['Price', 'Alcohol', 'Residual_Sugar', 'Sulphates', 'pH','CountryFrance']].values\n",
    "train_y = data['Rating'].values\n",
    "'''\n",
    "Fitting data to a regression model requires two arguments, the training X\n",
    "values (independent variables) and the training y values (dependent variables.\n",
    "In general, most fit functions for models follow this format.\n",
    "'''\n",
    "reg = LinearRegression().fit(train_X, train_y)\n",
    "print(\"R-Squared:\", reg.score(train_X, train_y))\n",
    "\n",
    "model = ols('Rating ~ Price + Alcohol + Sulphates + CountryFrance',data).fit()\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "#Testing for heteroskasticity \n",
    "\n",
    "#Residuals calculated by definition above.\n",
    "predicted_y = reg.predict(train_X)\n",
    "#Note we can perform element-wise subtraction between arrays like so\n",
    "residuals = train_y - predicted_y\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(predicted_y,residuals,s=2,c='black')\n",
    "\n",
    "#This line adds the dashed horizontal line\n",
    "plt.hlines(0,min(predicted_y),max(predicted_y),color='red',linestyles='dashed')\n",
    "\n",
    "plt.xlabel(\"Model Prediction\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.show()\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "\n",
    "#Import the relevant libraries and train the model\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "results = ols('Rating ~ Price + Alcohol  + Sulphates + pH',data).fit()\n",
    "\n",
    "#Perform the Breuch-Pagan Test by running this line\n",
    "bp = het_breuschpagan(results.resid,results.model.exog)\n",
    "measures = ('LM Statistic', 'LM-Test p-value', 'F-Statistic', 'F-Test p-value')\n",
    "print(dict(zip(measures,bp)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
