{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///:memory:')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your manager doesn’t know much about analytics. That is too bad, but don’t worry, you’ll have his \n",
    "job  soon  enough.  In  the  meantime,  he  has  made  the  following  comments.  Here  are  your  tasks  for  \n",
    "each of the below quotes: Identify whether his suggestions /comments are good or bad.  \n",
    "o If they are good, explain why. \n",
    "o If they are bad, explain why and what you should do to fix them or make them better. Make sure your answer demonstrates that you have a sophisticated understanding of the issues \n",
    "involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. a. “Ten percent of our customers did not complete a satisfaction survey. We can get most of their \n",
    "demographic information from an existing database, so we decided to complete our analysis \n",
    "substituting in the average response from other people for any missing values.”  \n",
    "\tbad \n",
    "\t•\tMissing Not at Random (MNAR): The fact that 10% of customers did not complete a satisfaction survey might be related to their level of satisfaction or dissatisfaction. For example, very dissatisfied or very satisfied customers might be less likely to respond, skewing the average responses if their data is not captured authentically.\n",
    "\n",
    "\n",
    "b. I was running a regression model – variable X1 worked fine until I added X2, then neither were \n",
    "significant, so I took X2 back out and left X1 by itself. \n",
    "\n",
    "\tbad\n",
    "\n",
    "\t•\tIgnoring Multicollinearity: When adding a variable X2 affects the significance of another variable X1, it often suggests the presence of multicollinearity—where X1 and X2 are highly correlated. Removing X2 without understanding why it impacts X1 could lead to a model that is biased or misses important explanatory power provided by X2.\n",
    "\t•\tModel Robustness and Causality Misinterpretation: Simply removing X2 because it makes X1 insignificant could be oversimplifying the model. This approach might ignore deeper relationships and interactions between variables, potentially leading to erroneous conclusions about causality and effects.\n",
    "\n",
    "    Suggested Improvement\n",
    "\n",
    "\t1.\tInvestigate Multicollinearity:\n",
    "\t•\tCorrelation Analysis: Check the correlation between X1 and X2. If the variables are highly correlated, consider methods to address this, like PCA (Principal Component Analysis) or partialling out the effect of one variable.\n",
    "\t•\tVariance Inflation Factor (VIF): Calculate VIF to quantify the severity of multicollinearity. A VIF value greater than 5 is often considered indicative of problematic multicollinearity.\n",
    "\n",
    "\n",
    "c. “The R-squared of a regression is generally a good indicator of how well the overall model fits \n",
    "the data.” \n",
    "\n",
    "\tQuality of Suggestion: Good.\n",
    "\n",
    "\tReason:\n",
    "\n",
    "\t•\tSurface Level Validity: On the surface, R-squared does measure the proportion of variance in the dependent variable that is predictable from the independent variables. It gives an intuitive measure of how much of the variability in the response data can be explained by the model. This makes it a useful statistic in many contexts.\n",
    "\t•\tLimitations and Misinterpretations: R-squared alone does not tell the whole story about the effectiveness of a regression model. It has several limitations and can be misleading if interpreted in isolation.\n",
    "\n",
    "    By understanding and using R-squared in conjunction with other analytical methods and metrics, you can develop a more accurate and comprehensive view of your model’s performance and its ability to predict or explain new data. This holistic approach to model evaluation will lead to more robust and reliable analytical insights.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\td.  “When developing a model from theory, it doesn’t matter whether you start with a small model \n",
    "\tand build up, or a large model and build down, you get to the same place – somewhere in the \n",
    "\tmiddle – either way.”  \n",
    "\n",
    "\tQuality of Suggestion: Partially Good, but Mostly Problematic.\n",
    "\n",
    "\tReason:\n",
    "\n",
    "\t\t•\tFlexibility in Approach: It’s true that both approaches—starting with a simple model and adding variables (forward selection), or starting with a full model and removing variables (backward elimination)—are valid methods in statistical modeling. Both aim to balance complexity and explanatory power.\n",
    "\t\t•\tPotential for Different Outcomes: However, the assumption that both approaches lead to the same model is optimistic and not always accurate. The path taken can influence the final model due to differences in how significance, multicollinearity, and interaction effects are handled during the model-building process.\n",
    "\n",
    "\tLimitations of the Comment\n",
    "\n",
    "\t\t1.\tDifferent Results Due to Statistical Nuances: Forward selection and backward elimination can lead to different final models because each step’s decisions depend on the current model’s state. For example, a variable that seems insignificant in a larger set might prove significant when fewer predictors are competing.\n",
    "\t\t2.\tOverfitting vs. Underfitting: Starting with too many variables might lead to an overfitted model, especially if not all variables are theoretically justified. Conversely, starting with too few might miss important predictors, leading to an underfitted model.\n",
    "\t\t3.\tThe Role of Model Assumptions: The development path might affect how well model assumptions (like linearity and independence of errors) hold, impacting model validity and generalizability.\n",
    "\n",
    "\tSuggested Improvement\n",
    "\n",
    "\t\t1.\tTheory-Driven Model Development: Begin with a model based on theoretical or domain knowledge rather than purely statistical criteria. This helps ensure that the variables included have a logical basis for their inclusion.\n",
    "\t\t2.\tUse Both Approaches in Parallel: To ascertain the robustness of the model, it might be useful to explore both forward and backward methods to see if they converge on a similar model. If they diverge, understanding why can provide deeper insights.\n",
    "\t\t3.\tIncorporate Cross-Validation: Regardless of the starting point, use cross-validation to assess the model’s predictive performance on unseen data, which helps in mitigating the risks of overfitting.\n",
    "\t\t4.\tConsider Model Complexity and Parsimony: Use information criteria like AIC or BIC, which penalize excessive model complexity, to help in deciding on the final model. These criteria can guide the addition or removal of variables in a more objective manner.\n",
    "\t\t5.\tRegularization Techniques: Employ regularization methods (like Lasso or Ridge) that can shrink coefficients or reduce them to zero, effectively performing variable selection during the estimation process.\n",
    "\n",
    "\tBy addressing these points, you can ensure that the model development process is both rigorous and theoretically justified, leading to a more reliable and effective model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Many university and college students work during their summer vacations.  One particular school was \n",
    "very interested in understanding student employment prospects.  To develop insights, a confidential \n",
    "survey was conducted for second-year students and that data was linked to records on their academic \n",
    "performance and other demographic information for those who worked in the summer.  \n",
    "You have been hired by the school’s principal to analyze the data.   In general he would like you to \n",
    "analyze student earnings in terms of their grade performance and demographic characteristics.  He \n",
    "has provided you with a list of questions below that he would like to have answered by your analysis.  \n",
    "For each question, he would like you to do whatever analysis is required to answer the questions \n",
    "and provide an explanation in language he is likely to understand.  He encourages you to read all of \n",
    "the parts of the question first. \n",
    "\n",
    "\n",
    "a. Develop a model of student earnings based on the data on tab ‘Students’.  Explain why you \n",
    "chose the model you did and what the results tell you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               Earnings   R-squared:                       0.699\n",
      "Model:                            OLS   Adj. R-squared:                  0.679\n",
      "Method:                 Least Squares   F-statistic:                     35.96\n",
      "Date:                Sun, 21 Jul 2024   Prob (F-statistic):           3.35e-22\n",
      "Time:                        20:20:49   Log-Likelihood:                -753.22\n",
      "No. Observations:                 100   AIC:                             1520.\n",
      "Df Residuals:                      93   BIC:                             1539.\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "====================================================================================\n",
      "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------\n",
      "Intercept         1765.7339   1494.769      1.181      0.241   -1202.581    4734.048\n",
      "Age                 86.2924     59.404      1.453      0.150     -31.672     204.257\n",
      "N_Summers_Worked   469.3834     42.812     10.964      0.000     384.368     554.399\n",
      "Grade_Calculus      35.0583      5.635      6.221      0.000      23.867      46.249\n",
      "Grade_English       -8.9217      9.261     -0.963      0.338     -27.313       9.470\n",
      "Grade_Accounting    14.7576      6.005      2.458      0.016       2.833      26.682\n",
      "Grade_OB             5.4121      3.654      1.481      0.142      -1.844      12.668\n",
      "==============================================================================\n",
      "Omnibus:                        0.955   Durbin-Watson:                   1.676\n",
      "Prob(Omnibus):                  0.620   Jarque-Bera (JB):                0.871\n",
      "Skew:                           0.225   Prob(JB):                        0.647\n",
      "Kurtosis:                       2.919   Cond. No.                     4.90e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 4.9e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "#Importing Data\n",
    "db_dir = os.getcwd()\n",
    "db_dir = db_dir + '/data/MMA_860_Final_Practice_Data_vf.xlsx'\n",
    "data = pd.read_excel(db_dir, sheet_name='Students')\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "train_X = data[['Age','N_Summers_Worked','Grade_Calculus','Grade_English','Grade_Accounting','Grade_OB']].values\n",
    "train_y = data['Earnings'].values\t\t\t\n",
    "reg = LinearRegression().fit(train_X, train_y)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "model = ols('Earnings ~ Age + N_Summers_Worked + Grade_Calculus + Grade_English + Grade_Accounting + Grade_OB',data).fit()\n",
    "print(model.summary())\n",
    "\n",
    "# The decrease in R-squared after removing variables with high p-values is a common observation. R-squared measures the proportion of variance in the dependent variable that is predictable from the independent variables. When you remove variables, regardless of their p-values, you’re likely to explain less of the variance unless those variables were entirely irrelevant or redundant. Here’s how you can address this situation:\n",
    "\n",
    "# Understanding the Impact of Variable Removal\n",
    "\n",
    "# \t1.\tR-squared Decrease: Even if the variables were not statistically significant, they might still carry some information that contributes to explaining the variance in earnings. Removing them reduces the complexity of the model, which can sometimes make the model less fit to capture subtle nuances in your data.\n",
    "# \t2.\tAdjusted R-squared: It’s crucial to look at the adjusted R-squared rather than just R-squared when removing variables. Adjusted R-squared adjusts for the number of variables in the model, providing a more accurate measure of model performance, especially in the context of model simplification.\n",
    "# \t3.\tTrade-offs: There’s always a trade-off between model simplicity and explanatory power. A simpler model (fewer variables) is easier to understand and less likely to overfit, but it may have less predictive power.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               Earnings   R-squared:                       0.682\n",
      "Model:                            OLS   Adj. R-squared:                  0.672\n",
      "Method:                 Least Squares   F-statistic:                     68.56\n",
      "Date:                Sun, 21 Jul 2024   Prob (F-statistic):           8.85e-24\n",
      "Time:                        20:12:25   Log-Likelihood:                -755.97\n",
      "No. Observations:                 100   AIC:                             1520.\n",
      "Df Residuals:                      96   BIC:                             1530.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "====================================================================================\n",
      "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------\n",
      "Intercept         3071.3095    402.533      7.630      0.000    2272.287    3870.332\n",
      "N_Summers_Worked   471.3899     42.527     11.084      0.000     386.973     555.806\n",
      "Grade_Calculus      38.3709      5.361      7.157      0.000      27.728      49.013\n",
      "Grade_Accounting    13.4655      5.849      2.302      0.023       1.856      25.075\n",
      "==============================================================================\n",
      "Omnibus:                        0.573   Durbin-Watson:                   1.711\n",
      "Prob(Omnibus):                  0.751   Jarque-Bera (JB):                0.377\n",
      "Skew:                           0.149   Prob(JB):                        0.828\n",
      "Kurtosis:                       3.034   Cond. No.                         891.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "model = ols('Earnings ~ N_Summers_Worked + Grade_Calculus + Grade_Accounting',data).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LM Statistic': 2.033270193824943, 'LM-Test p-value': 0.5655301463179954, 'F-Statistic': 0.6641504348581097, 'F-Test p-value': 0.5760956420622918}\n",
      "R-Squared on Test Data: 0.6987910582717095\n",
      "Mean Squared Error on Test Data: 204140.82825017723\n"
     ]
    }
   ],
   "source": [
    "# b. Is  heteroskedasticity  an  issue  with  this  model?    Regardless  of  your  findings,  assume  that  heteroskedasticity is not an issue for the balance of your analysis. \n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "# Assuming 'model' is the fitted OLS model from statsmodels\n",
    "residuals = model.resid\n",
    "predictions = model.fittedvalues\n",
    "\n",
    "model = ols('Earnings ~ N_Summers_Worked + Grade_Calculus + Grade_Accounting',data).fit()\n",
    "\n",
    "#Testing for heteroskasticity \n",
    "\n",
    "#Residuals calculated by definition above.\n",
    "predicted_y = reg.predict(train_X)\n",
    "#Note we can perform element-wise subtraction between arrays like so\n",
    "residuals = train_y - predicted_y\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(predicted_y,residuals,s=2,c='black')\n",
    "\n",
    "#This line adds the dashed horizontal line\n",
    "plt.hlines(0,min(predicted_y),max(predicted_y),color='red',linestyles='dashed')\n",
    "\n",
    "plt.xlabel(\"Model Prediction\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.show()\n",
    "\n",
    "#Perform the Breuch-Pagan Test by running this line\n",
    "bp = het_breuschpagan(model.resid,model.model.exog)\n",
    "measures = ('LM Statistic', 'LM-Test p-value', 'F-Statistic', 'F-Test p-value')\n",
    "print(dict(zip(measures,bp)))\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# Predictions on the test set\n",
    "y_pred_test = reg.predict(train_X)\n",
    "\n",
    "# Evaluating the model on the test data\n",
    "r_squared_test = r2_score(train_y, y_pred_test)\n",
    "mse_test = mean_squared_error(train_y, y_pred_test)\n",
    "\n",
    "print(\"R-Squared on Test Data:\", r_squared_test)\n",
    "print(\"Mean Squared Error on Test Data:\", mse_test)\n",
    "\n",
    "# Residual Plot Results\n",
    "\n",
    "# Observations from the Residual Plot\n",
    "\n",
    "# \t1.\tSpread of Residuals: The residuals do not show a clear pattern of increasing or decreasing spread as the model predictions increase. Ideally, in a homoscedastic dataset, the spread or variability of residuals should be roughly constant across all levels of predictions.\n",
    "# \t2.\tPresence of Outliers: There are a few points that lie far from the main cluster of residuals. While these outliers don’t necessarily indicate heteroskedasticity, they can influence the overall assessment and might need further investigation to determine their impact on the regression analysis.\n",
    "# \t3.\tHorizontal Line: The dashed horizontal line at zero helps visualize the central tendency of the residuals. Most residuals appear to cluster around this line without systematic patterns deviating from it as predictions change, which is a good sign.\n",
    "\n",
    "# Signs of Heteroskedasticity\n",
    "\n",
    "# \t•\tNo Apparent Funnel Shape or Pattern: There is no apparent funnel shape or clear pattern that would typically indicate heteroskedasticity, where the variance of residuals increases or decreases with the predicted values.\n",
    "# \t•\tRandom Scatter: The random scatter of residuals around the prediction line suggests that the variance is not changing systematically across the range of predictions, which is indicative of homoscedasticity.\n",
    "\n",
    "# Breusch-Pagan Test Results\n",
    "\n",
    "# \t•\tBreusch-Pagan Test: The p-value of the Breusch-Pagan test is approximately 0.576, which is above the common alpha level of 0.05. This means you fail to reject the null hypothesis of homoscedasticity. According to this test, there is not enough evidence to suggest that heteroskedasticity is a problem in your model.\n",
    "\n",
    "\t# •\tNo Clear Evidence of Heteroskedasticity: Both the visual and statistical tests indicate that heteroskedasticity may not be a significant issue in this model. The residuals do not show a clear pattern of variance, and the statistical test supports this observation.\n",
    "\t# •\tContinuing Analysis: As per your instructions to assume that heteroskedasticity is not an issue, you can proceed with further analysis without making adjustments for heteroskedasticity, such as using heteroskedasticity-consistent standard errors or transforming the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "c. The principal claims that summer earnings increase as grades in calculus increase.  If so, he \n",
    "would like to offer free tutoring support to students to increase their calculus grades so that \n",
    "they will earn more.  Does the evidence support the existence of such a relationship?  If such \n",
    "a relationship existed, would it justify his strategy? \n",
    "\n",
    "To assess the claim that summer earnings increase as grades in calculus increase, we need to look at the coefficient of the calculus grades variable from the regression model. According to your model results, the coefficient for \"Grade_Calculus\" is significantly positive (coefficient = 35.0583, p-value = 0.000). This suggests a statistically significant relationship where higher calculus grades are associated with higher earnings.\n",
    "\n",
    "### Evidence Supporting the Relationship\n",
    "\n",
    "- **Statistical Significance**: The coefficient for calculus grades is positive and highly significant, indicating that as calculus grades increase, so do earnings. This statistically supports the principal's claim.\n",
    "- **Magnitude of Impact**: The magnitude, a $35.06 increase in earnings for each one-point increase in calculus grade, suggests a practical significance, especially over the typical range of grade variation.\n",
    "\n",
    "### Justifying the Strategy\n",
    "\n",
    "If such a relationship exists and is believed to be causal, then improving calculus grades through free tutoring could theoretically lead to higher earnings. However, before implementing such a strategy, consider the following:\n",
    "\n",
    "1. **Causality vs. Correlation**: The key question is whether higher calculus grades cause higher earnings or if they are merely correlated due to other underlying factors (like overall academic ability, work ethic, or field of study that may also relate to higher-paying summer jobs). The regression analysis alone does not establish causality.\n",
    "\n",
    "2. **Costs vs. Benefits**: The principal would need to evaluate the cost of providing free tutoring and weigh it against the potential increase in student earnings. This analysis should include the number of students likely to use the service, the cost of hiring tutors, and the expected increase in earnings.\n",
    "\n",
    "3. **Generalizability**: The findings are based on this particular dataset and student body. Before generalizing this strategy to other contexts or over different time periods, it would be prudent to validate these results further.\n",
    "\n",
    "4. **Alternative Strategies**: Consider whether there are more cost-effective or impactful ways to achieve similar outcomes. For instance, improving educational outcomes more broadly or supporting other skills that are also highly valued in the job market.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The evidence from your regression model supports the principal's claim that higher calculus grades are associated with higher summer earnings. If the principal believes that the observed relationship could be causal, and if the costs of tutoring are justified by the potential earnings increase, then his strategy could be deemed reasonable.\n",
    "\n",
    "However, before proceeding, it is advisable to conduct further analysis to establish causality and to perform a detailed cost-benefit analysis to ensure the financial viability and overall effectiveness of the tutoring program. Additionally, exploring other factors that could influence earnings and considering broader educational initiatives could provide more holistic benefits to students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               Earnings   R-squared:                       0.728\n",
      "Model:                            OLS   Adj. R-squared:                  0.694\n",
      "Method:                 Least Squares   F-statistic:                     21.42\n",
      "Date:                Sun, 21 Jul 2024   Prob (F-statistic):           2.03e-20\n",
      "Time:                        20:23:20   Log-Likelihood:                -748.11\n",
      "No. Observations:                 100   AIC:                             1520.\n",
      "Df Residuals:                      88   BIC:                             1551.\n",
      "Df Model:                          11                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================================\n",
      "                                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Intercept                           907.2298   1015.332      0.894      0.374   -1110.530    2924.990\n",
      "Gender_M[T.True]                   1548.7218    991.072      1.563      0.122    -420.826    3518.269\n",
      "Gender_F[T.True]                   -641.4919   1060.846     -0.605      0.547   -2749.701    1466.717\n",
      "Age                                  95.0110     60.408      1.573      0.119     -25.038     215.060\n",
      "N_Summers_Worked                    445.3028     44.035     10.112      0.000     357.792     532.814\n",
      "Grade_Calculus                       23.7410      3.767      6.303      0.000      16.255      31.227\n",
      "Gender_M[T.True]:Grade_Calculus      17.1428      6.319      2.713      0.008       4.585      29.701\n",
      "Gender_F[T.True]:Grade_Calculus       6.5982      5.580      1.183      0.240      -4.490      17.686\n",
      "Grade_English                        -3.7609      6.167     -0.610      0.544     -16.016       8.494\n",
      "Gender_M[T.True]:Grade_English      -22.8630      9.352     -2.445      0.016     -41.448      -4.278\n",
      "Gender_F[T.True]:Grade_English       19.1021     10.195      1.874      0.064      -1.159      39.363\n",
      "Grade_Accounting                     10.1513      3.980      2.551      0.012       2.242      18.061\n",
      "Gender_M[T.True]:Grade_Accounting     2.0256      6.461      0.314      0.755     -10.814      14.866\n",
      "Gender_F[T.True]:Grade_Accounting     8.1257      5.866      1.385      0.169      -3.531      19.782\n",
      "Grade_OB                              3.0552      2.570      1.189      0.238      -2.053       8.163\n",
      "Gender_M[T.True]:Grade_OB             6.0924      4.540      1.342      0.183      -2.930      15.115\n",
      "Gender_F[T.True]:Grade_OB            -3.0372      3.859     -0.787      0.433     -10.707       4.632\n",
      "==============================================================================\n",
      "Omnibus:                        2.793   Durbin-Watson:                   1.752\n",
      "Prob(Omnibus):                  0.247   Jarque-Bera (JB):                2.160\n",
      "Skew:                           0.276   Prob(JB):                        0.340\n",
      "Kurtosis:                       3.463   Cond. No.                     1.14e+18\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 2.76e-30. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "# d. Is  there  evidence  of  any  difference  in  performance  between  male  and  female  students  in  \n",
    "# terms of the relationship between at least some of their grades and their summer earnings?\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Adding interaction terms between Gender and Grade variables\n",
    "data1 = pd.get_dummies(data, columns=['Gender']) \n",
    "\n",
    "# data1\n",
    "model_with_interactions = ols( 'Earnings ~ Age + N_Summers_Worked + Grade_Calculus + Grade_English + Grade_Accounting + Grade_OB + Gender_M + Gender_F + Gender_M:Grade_Calculus + Gender_M:Grade_English + Gender_M:Grade_Accounting + Gender_M:Grade_OB + Gender_F:Grade_Calculus + Gender_F:Grade_English + Gender_F:Grade_Accounting + Gender_F:Grade_OB', data=data1).fit()\n",
    "\n",
    "print(model_with_interactions.summary())\n",
    "\n",
    "# there is evidence that there is significance between male and the grade they got in english and Calculus to influence there summer earnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. A Kingston real estate company believes that the most important factor in selling a house is setting \n",
    "the initial price correctly.  This company has hired you as a consultant to develop a model to predict \n",
    "housing prices so they can set initial prices accordingly.  A collection of data has been provided to you \n",
    "on  the  tab  ‘housing’.    The  client  insists  that  the  model  should  include  all  the  explanatory  variables  \n",
    "they have provided: Price = B0 + B1 N_Bedrooms + B2 N_Bathrooms + B3 House_Size + B4 Age + B5 \n",
    "Renovated_Kitchen + B6 Finished_Basement + B7 Close_to_Campus.  \n",
    "\n",
    "\n",
    "a. In principle, how would you look for outliers in the data?  If you found any how would you \n",
    "recommend that the client deal with them? Note: you do not need to actually do anything for \n",
    "this question – there are no outliers in the data. \n",
    "\n",
    "\t1.\tVisual Inspection:\n",
    "\t•\tBox Plots: These are especially useful for quickly visualizing where the bulk of data lies and identifying points that fall far outside the common range. Each variable (e.g., N_Bedrooms, House_Size) can be plotted separately.\n",
    "\t•\tScatter Plots: Useful for detecting outliers in the context of two variables (e.g., House_Size vs. Price). This can help identify anomalies that are not evident when examining a single variable.\n",
    "\n",
    "    2.\tStatistical Methods:\n",
    "\t•\tZ-Scores: Calculate the Z-scores of the data points, which indicate how many standard deviations a point is from the mean. Points with a Z-score greater than 3 or less than -3 are typically considered outliers.\n",
    "\t•\tInterquartile Range (IQR): The IQR is the difference between the 75th percentile (Q3) and the 25th percentile (Q1) of a dataset. Data points that fall below  Q1 - 1.5 \\times IQR  or above  Q3 + 1.5 \\times IQR  are often regarded as outliers.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming data is loaded into a DataFrame `df`\n",
    "Q1 = df['House_Size'].quantile(0.25)\n",
    "Q3 = df['House_Size'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out potential outliers\n",
    "outliers = df[(df['House_Size'] < lower_bound) | (df['House_Size'] > upper_bound)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "e. Suppose I want to sell my house.  Does your first model suggest that I should add a bathroom \n",
    "at  a  cost  of  $3,000  and  that  doing  so  would  be  justified  by  the  resulting  increase  in  selling  \n",
    "price?  Be sure to justify your answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. If there had been missing data elements, how would you recommend the client deal with that \n",
    "in their data?  Note: you do not need to actually do anything for this question – there are no \n",
    "missing values in the data. \n",
    "\n",
    "When faced with missing data in a dataset, it's crucial to have a well-thought-out strategy for addressing the gaps before proceeding with analysis or modeling. The approach depends on the nature of the missing data, the amount missing, and how it is likely to impact the analysis. Here are several common strategies for handling missing data, each with its strengths and situations where it is most applicable:\n",
    "\n",
    "### 1. **Deletion Methods**\n",
    "- **Listwise Deletion**: Remove any record (row) from the dataset that contains a missing value. This method is straightforward but can lead to a significant reduction in dataset size, potentially biasing the results if the missingness is not completely random.\n",
    "- **Pairwise Deletion**: Use available data while ignoring the missing values during analysis. This approach can be used in correlation or regression analyses but can lead to differing sample sizes being used for different calculations, complicating the interpretation.\n",
    "\n",
    "### 2. **Imputation Methods**\n",
    "- **Mean/Median/Mode Imputation**: Replace missing values with the mean, median, or mode (most frequent value) of the observed data in the same variable. This method is simple but can reduce the variability in the dataset.\n",
    "- **Regression Imputation**: Use a regression model to predict and fill in missing values based on other variables in the dataset.\n",
    "- **Multiple Imputation**: As previously discussed, this method involves creating several different plausible imputations for missing values, conducting analysis on each of these datasets, and combining the results. It effectively captures the uncertainty of the missing data.\n",
    "\n",
    "### 3. **Model-Based Methods**\n",
    "- **Maximum Likelihood Estimation (MLE)**: Use a likelihood-based approach where parameters are estimated directly from the data that are observed while accounting for the missingness. This method is often used in more complex statistical analyses.\n",
    "- **Bayesian Methods**: Similar to MLE but incorporates prior distributions over parameters, which can be especially useful if there is prior knowledge about the parameters.\n",
    "\n",
    "### 4. **Data Augmentation**\n",
    "- **Interpolation and Extrapolation**: These techniques are particularly useful in time-series data where missing values can be estimated from trends.\n",
    "- **Last Observation Carried Forward (LOCF)**: Fill in missing values with the last observed value. Common in time-series data but can introduce bias.\n",
    "\n",
    "### Recommendations to Clients\n",
    "\n",
    "- **Assess the Mechanism of Missingness**: Determine if data are missing completely at random (MCAR), missing at random (MAR), or missing not at random (MNAR). This assessment will guide the choice of the method to handle missing data.\n",
    "- **Consider the Proportion and Patterns of Missing Data**: Large amounts of missing data or certain patterns might necessitate more sophisticated approaches like multiple imputation or model-based methods.\n",
    "- **Evaluate the Impact on Analysis**: Whatever method is chosen, it's important to assess how the method impacts the results of the analysis. Sensitivity analysis can be helpful here.\n",
    "- **Documentation and Transparency**: Clearly document the methods used for handling missing data and the reasons for their choice. This practice is crucial for the integrity and reproducibility of the analysis.\n",
    "\n",
    "By providing these guidelines, you help ensure that the real estate company can handle missing data effectively, preserving the validity and reliability of their analyses and decision-making processes based on the dataset provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  Price   R-squared:                       0.453\n",
      "Model:                            OLS   Adj. R-squared:                  0.406\n",
      "Method:                 Least Squares   F-statistic:                     9.700\n",
      "Date:                Sun, 21 Jul 2024   Prob (F-statistic):           9.67e-09\n",
      "Time:                        20:40:25   Log-Likelihood:                -1129.2\n",
      "No. Observations:                  90   AIC:                             2274.\n",
      "Df Residuals:                      82   BIC:                             2294.\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "Intercept            8.7e+04   5.41e+04      1.607      0.112   -2.07e+04    1.95e+05\n",
      "N_Bedrooms         1.656e+04   1.22e+04      1.354      0.179   -7763.618    4.09e+04\n",
      "N_Bathrooms          2.9e+04   1.27e+04      2.291      0.025    3822.703    5.42e+04\n",
      "House_Size           54.1223     38.652      1.400      0.165     -22.769     131.014\n",
      "Age                -172.3439    498.685     -0.346      0.731   -1164.388     819.700\n",
      "Renovated_Kitchen   3.03e+04   2.12e+04      1.427      0.157    -1.2e+04    7.26e+04\n",
      "Finished_Basement -3477.6953   1.57e+04     -0.222      0.825   -3.47e+04    2.77e+04\n",
      "Close_to_Campus    7.464e+04    2.1e+04      3.553      0.001    3.28e+04    1.16e+05\n",
      "==============================================================================\n",
      "Omnibus:                        2.628   Durbin-Watson:                   1.932\n",
      "Prob(Omnibus):                  0.269   Jarque-Bera (JB):                2.500\n",
      "Skew:                          -0.403   Prob(JB):                        0.287\n",
      "Kurtosis:                       2.868   Cond. No.                     1.62e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.62e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# c. Aside  from  the  size  variables  House_Size  and  N_Bathrooms,  do  all  the  variables  appear  to  \n",
    "# belong?  If not, which would you remove if the client did not insist on including them?  What \n",
    "# is the consequence of including irrelevant variables?\n",
    "\n",
    "#Importing Data\n",
    "db_dir = os.getcwd()\n",
    "db_dir = db_dir + '/data/MMA_860_Final_Practice_Data_vf.xlsx'\n",
    "data = pd.read_excel(db_dir, sheet_name='3.KingstonRealEstate')\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "train_X = data[['N_Bedrooms','N_Bathrooms','House_Size','Age','Renovated_Kitchen','Finished_Basement','Close_to_Campus']].values\n",
    "train_y = data['Price'].values\t\t\t\n",
    "reg = LinearRegression().fit(train_X, train_y)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "model = ols('Price ~ N_Bedrooms+N_Bathrooms+House_Size+Age+Renovated_Kitchen+Finished_Basement+Close_to_Campus',data).fit()\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the regression summary provided, we can analyze whether all the variables in the model appear to belong, and discuss the potential consequences of including irrelevant variables.\n",
    "\n",
    "### Analysis of the Regression Output\n",
    "The summary shows the coefficients, standard errors, t-statistics, and p-values for each predictor in the model. Let's evaluate each variable based on its p-value, which helps determine statistical significance (commonly, a p-value less than 0.05 is considered significant):\n",
    "\n",
    "1. **N_Bedrooms**: p-value = 0.179. This is not statistically significant at the 0.05 level.\n",
    "2. **N_Bathrooms**: p-value = 0.025. This is statistically significant.\n",
    "3. **House_Size**: p-value = 0.165. This is not statistically significant.\n",
    "4. **Age**: p-value = 0.731. This is not statistically significant.\n",
    "5. **Renovated_Kitchen**: p-value = 0.157. This is not statistically significant.\n",
    "6. **Finished_Basement**: p-value = 0.825. This is not statistically significant.\n",
    "7. **Close_to_Campus**: p-value = 0.001. This is highly statistically significant.\n",
    "\n",
    "### Variables to Consider Removing\n",
    "- **Age**, **Renovated_Kitchen**, **Finished_Basement**, and potentially **House_Size** and **N_Bedrooms** could be considered for removal if not insisted by the client. These variables have high p-values, indicating that they do not contribute significantly to explaining the variability in housing prices, given the other variables in the model.\n",
    "\n",
    "### Consequences of Including Irrelevant Variables\n",
    "Including variables that do not significantly contribute to the explanation of the dependent variable can have several adverse effects:\n",
    "- **Reduced Model Efficiency**: More variables mean more complexity, which can slow down computation and analysis, especially with larger datasets.\n",
    "- **Increased Risk of Overfitting**: Adding too many variables, especially those not relevant, can lead to a model that fits the noise in your training data rather than the actual underlying relationship. This can degrade the model’s performance on new, unseen data.\n",
    "- **Dilution of Other Variable Effects**: Unnecessary variables can obscure the effects of truly significant predictors because they add noise and variability to the model that isn’t helpful.\n",
    "- **Inflated Standard Errors**: More variables can increase the standard errors of the estimate coefficients, which makes it harder to detect true effects (as seen with the non-significant p-values).\n",
    "\n",
    "### Recommendations\n",
    "Given the non-significance of several variables, and depending on the context or domain knowledge (e.g., the importance of age or renovations in market dynamics), you might recommend to the client:\n",
    "- **Model Simplification**: Removing non-significant variables to make the model simpler and more interpretable.\n",
    "- **Refinement Through Additional Data or Variables**: Sometimes, non-significance can result from poor data quality, insufficient data, or the need for more nuanced variables that capture the effect more accurately.\n",
    "- **Reevaluation of Model Assumptions**: Check for proper functional forms, interaction effects, or non-linear relationships that might better capture the influence of these variables.\n",
    "\n",
    "Ultimately, a leaner model that retains only statistically significant variables (unless theory strongly supports their inclusion) will likely perform better in terms of both understanding and predicting housing prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. The client believes that the variables House_Size and N_Bathrooms should both be important \n",
    "in determining the selling price of homes.   \n",
    "\n",
    "i. Do these variables appear to belong in the model according to the individual t-tests?  \n",
    "From the regression summary provided:\n",
    "\n",
    "\t•\tHouse_Size: It has a p-value of 0.165, which suggests it is not statistically significant at the usual 0.05 level in this particular model configuration.\n",
    "\t•\tN_Bathrooms: It has a p-value of 0.025, indicating it is statistically significant and suggesting it is an important predictor of the selling price of homes.\n",
    "\n",
    "Based on these individual t-tests, N_Bathrooms appears to belong in the model, while House_Size does not seem as crucial in the given model setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<F test: F=4.729271580176495, p=0.011380792336496132, df_denom=82, df_num=2>\n"
     ]
    }
   ],
   "source": [
    "# ii. Perform a test to see if any ‘questionable’ variables jointly belong in the model.  What \n",
    "# is the most likely explanation for the results? \n",
    "\n",
    "# To check if the “questionable” variables jointly contribute to the model, you can perform an F-test. This test will help you assess whether these variables, when considered together, have a significant joint effect on the dependent variable. In Python, this can be done using statsmodels.\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Assuming you have a fitted model 'model'\n",
    "hypotheses = '(House_Size = 0), (N_Bathrooms = 0)'\n",
    "f_test = model.f_test(hypotheses)\n",
    "print(f_test)\n",
    "\n",
    "# The F-test assesses whether there’s a significant joint effect of House_Size and N_Bathrooms on the dependent variable, which in this case is the house price.\n",
    "\n",
    "# Conclusions from the F-Test:\n",
    "\n",
    "# \t•\tStatistical Significance: The p-value is 0.0114, which is less than the typical alpha level of 0.05. This result indicates that you can reject the null hypothesis that both House_Size and N_Bathrooms have no joint effect on the selling price of homes.\n",
    "# \t•\tImportance of Variables: Despite House_Size not being significant individually (as indicated by its individual p-value of 0.165), the F-test suggests that when combined with N_Bathrooms, these variables do have a significant joint effect on house prices. This might indicate that while the effect of House_Size might not be strong on its own, its contribution to the model in conjunction with N_Bathrooms is significant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  Price   R-squared:                       0.440\n",
      "Model:                            OLS   Adj. R-squared:                  0.399\n",
      "Method:                 Least Squares   F-statistic:                     10.86\n",
      "Date:                Sun, 21 Jul 2024   Prob (F-statistic):           6.79e-09\n",
      "Time:                        21:06:17   Log-Likelihood:                -1130.3\n",
      "No. Observations:                  90   AIC:                             2275.\n",
      "Df Residuals:                      83   BIC:                             2292.\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "Intercept          1.462e+05    3.4e+04      4.296      0.000    7.85e+04    2.14e+05\n",
      "N_Bedrooms         3.071e+04   6917.856      4.439      0.000    1.69e+04    4.45e+04\n",
      "N_Bathrooms        3.351e+04   1.23e+04      2.723      0.008    9030.130     5.8e+04\n",
      "Age                -159.3850    501.477     -0.318      0.751   -1156.802     838.032\n",
      "Renovated_Kitchen  2.559e+04   2.11e+04      1.213      0.228   -1.64e+04    6.75e+04\n",
      "Finished_Basement   397.8178   1.55e+04      0.026      0.980   -3.05e+04    3.13e+04\n",
      "Close_to_Campus    8.199e+04   2.05e+04      4.007      0.000    4.13e+04    1.23e+05\n",
      "==============================================================================\n",
      "Omnibus:                        4.506   Durbin-Watson:                   2.007\n",
      "Prob(Omnibus):                  0.105   Jarque-Bera (JB):                4.332\n",
      "Skew:                          -0.536   Prob(JB):                        0.115\n",
      "Kurtosis:                       2.936   Cond. No.                         145.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# iii. If you ran the model without House_Size, what happens to the apparent significance \n",
    "# of N_Bedrooms?  Would this make for a better model? \n",
    "\n",
    "model = ols('Price ~ N_Bedrooms+N_Bathrooms+Age+Renovated_Kitchen+Finished_Basement+Close_to_Campus',data).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the updated model results without `House_Size`, we see some notable changes in the coefficients and the statistical significance of other variables, particularly `N_Bedrooms`.\n",
    "\n",
    "### Analysis of the Model without House_Size\n",
    "\n",
    "1. **Significance of `N_Bedrooms`**:\n",
    "   - **Coefficient**: 3.71e+04\n",
    "   - **P-value**: 0.000\n",
    "   - The significance of `N_Bedrooms` has increased dramatically in the absence of `House_Size`. It now shows a highly significant p-value, indicating a strong relationship between the number of bedrooms and the price. This suggests that `N_Bedrooms` might be capturing some of the effects previously attributed to `House_Size`.\n",
    "\n",
    "2. **Model Fit**:\n",
    "   - **R-squared**: 0.440\n",
    "   - **Adjusted R-squared**: 0.399\n",
    "   - These values indicate that the model explains 44% of the variance in house prices, with a slight adjustment for the number of predictors. While the fit is reasonable, it's essential to compare this to the R-squared from the model including `House_Size` to fully evaluate the impact of removing this variable.\n",
    "\n",
    "### Would This Make for a Better Model?\n",
    "\n",
    "The question of whether this constitutes a better model depends on several factors:\n",
    "\n",
    "- **Simplicity vs. Explanatory Power**: Removing `House_Size` simplifies the model, which can be beneficial if the goal is to minimize complexity without significantly sacrificing model accuracy. However, the slight drop in R-squared suggests that `House_Size` does contribute valuable information.\n",
    "- **Risk of Omitting Relevant Variables**: `House_Size` could be capturing aspects of the property's value not encapsulated by the number of bedrooms or bathrooms alone, such as the spaciousness of common areas, which might be important to prospective buyers.\n",
    "- **Interpretation and Use of Model**: If the model's purpose is to aid in setting initial pricing accurately and comprehensively, excluding an apparently relevant variable like `House_Size` might not serve the best interests of the client, unless the correlation between `House_Size` and `N_Bedrooms` is so strong that the latter adequately represents both.\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "- **Statistical Test for Comparison**: Consider conducting a formal statistical test, such as an F-test, to compare the fits of the models with and without `House_Size`. This can provide a more objective basis for deciding whether the reduced model is preferable.\n",
    "- **Consider Practical Implications**: From a practical standpoint, both `House_Size` and `N_Bedrooms` are important for buyers and can independently influence purchasing decisions. Excluding `House_Size` might overlook some nuances in how space is valued differently across various property types.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "While the model without `House_Size` shows increased significance for `N_Bedrooms`, suggesting that it captures a significant part of the variation in house prices, the overall decrease in explanatory power (R-squared) and the potential loss of information about the property's appeal suggest that including `House_Size` might still be the better choice for achieving a comprehensive and accurate pricing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated increase in selling price: $29001.02\n",
      "Cost of adding a bathroom: $3000\n",
      "Net gain from adding a bathroom: $26001.02\n",
      "Adding a bathroom is economically justified.\n"
     ]
    }
   ],
   "source": [
    "# e. Suppose I want to sell my house.  Does your first model suggest that I should add a bathroom \n",
    "# at  a  cost  of  $3,000  and  that  doing  so  would  be  justified  by  the  resulting  increase  in  selling  \n",
    "# price?  Be sure to justify your answer. \n",
    "\n",
    "import statsmodels.api as sm\n",
    "model = ols('Price ~ N_Bedrooms+N_Bathrooms+House_Size+Age+Renovated_Kitchen+Finished_Basement+Close_to_Campus',data).fit()\n",
    "# Assume 'model' is already fitted with your data\n",
    "# Coefficient for N_Bathrooms\n",
    "coef_bathrooms = model.params['N_Bathrooms']\n",
    "\n",
    "# Calculate the increase in price from adding one bathroom\n",
    "increase_in_price = coef_bathrooms * 1  # because you add one bathroom\n",
    "\n",
    "# Cost of adding the bathroom\n",
    "cost_of_addition = 3000\n",
    "\n",
    "# Net gain from adding the bathroom\n",
    "net_gain = increase_in_price - cost_of_addition\n",
    "\n",
    "print(f\"Estimated increase in selling price: ${increase_in_price:.2f}\")\n",
    "print(f\"Cost of adding a bathroom: ${cost_of_addition}\")\n",
    "print(f\"Net gain from adding a bathroom: ${net_gain:.2f}\")\n",
    "\n",
    "# Decision based on net gain\n",
    "if net_gain > 0:\n",
    "    print(\"Adding a bathroom is economically justified.\")\n",
    "else:\n",
    "    print(\"Adding a bathroom is not economically justified.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
