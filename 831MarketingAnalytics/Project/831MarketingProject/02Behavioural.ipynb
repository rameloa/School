{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team Gordon\n",
    "\n",
    "Student Name\tStudent Number\n",
    " Alisha Sahota\t20497348\n",
    " Anthony Ramelo\t20499391\n",
    " Chris Wu\t10182394\n",
    " Elizabeth Zhang\t20161231\n",
    " Emily Zhao\t10096273\n",
    " Sam Hossain\t20466500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def load_data(file_path, sheet_name):\n",
    "    \"\"\"\n",
    "    Load data from an Excel file and strip column names.\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    df.columns = df.columns.str.strip()  # Remove extra spaces\n",
    "    return df\n",
    "\n",
    "def ensure_column_exists(df, column_name, alternative_names=None):\n",
    "    \"\"\"\n",
    "    Ensure that a column exists in the DataFrame, optionally checking alternative names.\n",
    "    \"\"\"\n",
    "    if column_name in df.columns:\n",
    "        return column_name\n",
    "    elif alternative_names:\n",
    "        for alt_name in alternative_names:\n",
    "            if alt_name in df.columns:\n",
    "                return alt_name\n",
    "    raise ValueError(f\"Column '{column_name}' or alternatives {alternative_names} not found.\")\n",
    "\n",
    "def create_financial_features(df):\n",
    "    \"\"\"\n",
    "    Create financial and time-based features to enhance clustering.\n",
    "    \"\"\"\n",
    "    # Current Year\n",
    "    current_year = datetime.now().year\n",
    "\n",
    "    # Debt-to-Income Ratio\n",
    "    df['Debt_to_Income_Ratio'] = df['Outstanding Principal'] / df['Qualified / Verified\\nIncome']\n",
    "    \n",
    "    # Remaining Loan Percentage\n",
    "    df['Remaining_Loan_Percentage'] = df['Outstanding Balance'] / df['Loan Amount']\n",
    "    \n",
    "    # Loan-to-Income Ratio\n",
    "    df['Loan_to_Income_Ratio'] = df['Loan Amount'] / df['Qualified / Verified\\nIncome']\n",
    "    \n",
    "    # Monthly Repayment Burden\n",
    "    df['Monthly_Repayment_Burden'] = df['Outstanding Balance'] / df['Loan Term (Months)']\n",
    "    \n",
    "    # Consistency Score\n",
    "    df['Consistency_Score'] = df['Average activities per day'] / df['Average total activities per month']\n",
    "    \n",
    "    # Customer Age\n",
    "    df['Date of Birth'] = pd.to_datetime(df['Date of Birth'], errors='coerce')\n",
    "    df['Age'] = current_year - df['Date of Birth'].dt.year\n",
    "\n",
    "    # Loan Tenure Remaining\n",
    "    df['Disbursement Date'] = pd.to_datetime(df['Disbursement Date'], errors='coerce')\n",
    "    df['Elapsed_Months'] = (datetime.now() - df['Disbursement Date']).dt.days // 30\n",
    "    df['Remaining_Tenure'] = df['Loan Term (Months)'] - df['Elapsed_Months']\n",
    "    \n",
    "    # Handle infinite or NaN values (e.g., divide-by-zero cases)\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    print(\"Financial and time-based features created successfully.\")\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df, total_activities_col, columns_to_remove):\n",
    "    \"\"\"\n",
    "    Preprocess the data by dropping unnecessary columns, handling missing values,\n",
    "    encoding categorical variables, and scaling features.\n",
    "    \"\"\"\n",
    "    # Save the ID column separately for reference\n",
    "    df['ID'] = df['ID'].astype(str)\n",
    "    \n",
    "    # Preserve original dataset for reference\n",
    "    df_original = df.copy()\n",
    "    \n",
    "    # Remove unnecessary columns\n",
    "    existing_columns_to_remove = [col for col in columns_to_remove if col in df.columns]\n",
    "    df = df.drop(columns=existing_columns_to_remove)\n",
    "    \n",
    "    # Drop columns with more than 50% missing data\n",
    "    threshold = len(df) * 0.5\n",
    "    df = df.dropna(thresh=threshold, axis=1)\n",
    "    \n",
    "    # Handle missing values\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    imputer_num = SimpleImputer(strategy='mean')\n",
    "    imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "    \n",
    "    if not numeric_cols.empty:\n",
    "        df[numeric_cols] = pd.DataFrame(\n",
    "            imputer_num.fit_transform(df[numeric_cols]),\n",
    "            columns=numeric_cols,\n",
    "            index=df.index\n",
    "        )\n",
    "    \n",
    "    if not categorical_cols.empty:\n",
    "        df[categorical_cols] = pd.DataFrame(\n",
    "            imputer_cat.fit_transform(df[categorical_cols]),\n",
    "            columns=categorical_cols,\n",
    "            index=df.index\n",
    "        )\n",
    "    \n",
    "    # One-Hot Encoding specifically for Loan Status\n",
    "    if 'Loan Status' in df.columns:\n",
    "        df = pd.get_dummies(df, columns=['Loan Status'], drop_first=False)  # Keep all binary columns for loan statuses\n",
    "\n",
    "    # Binary Encoding for Gender\n",
    "    if 'Gender' in df.columns:\n",
    "        df['Gender'] = df['Gender'].map({'MALE': 0, 'FEMALE': 1})\n",
    "\n",
    "    # One-Hot Encoding for other categorical columns\n",
    "    categorical_columns_to_encode = ['Province', 'Residential Status']\n",
    "    columns_to_encode = [col for col in categorical_columns_to_encode if col in df.columns]\n",
    "    df = pd.get_dummies(df, columns=columns_to_encode, drop_first=True)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    df[df.select_dtypes(include=['float64', 'int64']).columns] = scaler.fit_transform(df.select_dtypes(include=['float64', 'int64']))\n",
    "    \n",
    "    print(\"Processed data (head):\")\n",
    "    print(df.head())\n",
    "    return df, df_original\n",
    "\n",
    "def perform_pca(df_encoded, variance_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Perform PCA on the encoded data to reduce dimensionality while retaining specified variance.\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=variance_threshold, random_state=42)\n",
    "    df_pca = pd.DataFrame(pca.fit_transform(df_encoded))\n",
    "    print(f\"Number of PCA components selected to retain {variance_threshold*100}% variance: {df_pca.shape[1]}\")\n",
    "    print(\"Explained variance by PCA components:\")\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    return df_pca\n",
    "\n",
    "def determine_optimal_clusters(df_pca, k_range=range(2, 8)):\n",
    "    \"\"\"\n",
    "    Determine the optimal number of clusters using the silhouette score.\n",
    "    \"\"\"\n",
    "    silhouette_scores = []\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=20)\n",
    "        labels = kmeans.fit_predict(df_pca)\n",
    "        silhouette_avg = silhouette_score(df_pca, labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "        print(f\"Silhouette Score for k={k}: {silhouette_avg:.4f}\")\n",
    "    \n",
    "    optimal_k = k_range[silhouette_scores.index(max(silhouette_scores))]\n",
    "    print(f\"Optimal number of clusters: {optimal_k}\")\n",
    "    print(f\"Highest Silhouette Score: {max(silhouette_scores):.4f}\")\n",
    "    return optimal_k\n",
    "\n",
    "def perform_clustering(df_pca, df_original, optimal_k):\n",
    "    \"\"\"\n",
    "    Perform KMeans clustering with the optimal number of clusters and assign cluster labels.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=20)\n",
    "    df_original['Cluster'] = kmeans.fit_predict(df_pca)\n",
    "    return df_original\n",
    "\n",
    "def save_clustered_data(df_original, output_dir=\"output\", filename=\"final_output_with_clusters.csv\"):\n",
    "    \"\"\"\n",
    "    Save the clustered dataset to a CSV file.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    df_original.to_csv(output_path, index=False)\n",
    "    print(f\"Clustered data saved to {output_path}\")\n",
    "\n",
    "def plot_clusters(df_pca, df_original, total_activities_col, output_dir=\"output\"):\n",
    "    \"\"\"\n",
    "    Create and save cluster visualization plots.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Scatter Plot of Clusters\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(df_pca.iloc[:, 0], df_pca.iloc[:, 1], c=df_original['Cluster'], cmap='viridis', alpha=0.6)\n",
    "    plt.title('Scatter Plot of Clusters')\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    scatter_plot_path = os.path.join(output_dir, \"scatter_plot_clusters.png\")\n",
    "    plt.savefig(scatter_plot_path)\n",
    "    plt.show()\n",
    "    print(f\"Scatter plot saved to {scatter_plot_path}\")\n",
    "    \n",
    "    # Cluster Counts\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cluster_counts = df_original['Cluster'].value_counts().sort_index()\n",
    "    sns.barplot(x=cluster_counts.index, y=cluster_counts.values, palette='viridis')\n",
    "    plt.title('Cluster Counts')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Count')\n",
    "    cluster_counts_path = os.path.join(output_dir, \"cluster_counts.png\")\n",
    "    plt.savefig(cluster_counts_path)\n",
    "    plt.show()\n",
    "    print(f\"Cluster counts plot saved to {cluster_counts_path}\")\n",
    "    \n",
    "    # Box Plot of Total Activities by Cluster\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=df_original, x='Cluster', y=total_activities_col, palette='viridis')\n",
    "    plt.title(f'Box Plot of {total_activities_col} by Cluster')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel(total_activities_col)\n",
    "    boxplot_path = os.path.join(output_dir, \"boxplot_total_activities_by_cluster.png\")\n",
    "    plt.savefig(boxplot_path)\n",
    "    plt.show()\n",
    "    print(f\"Box plot saved to {boxplot_path}\")\n",
    "\n",
    "def main():\n",
    "    # File and sheet details\n",
    "    file_path = 'Data 3 - October, 2024.xlsx'\n",
    "    sheet_name = 'Goals_and_plans_ALLDATA_202410'\n",
    "    \n",
    "    # Load the data\n",
    "    df = load_data(file_path, sheet_name)\n",
    "    df\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
