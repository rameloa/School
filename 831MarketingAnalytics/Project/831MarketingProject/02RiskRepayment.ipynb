{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team Gordon\n",
    "\n",
    "Student Name\tStudent Number\n",
    " Alisha Sahota\t20497348\n",
    " Anthony Ramelo\t20499391\n",
    " Chris Wu\t10182394\n",
    " Elizabeth Zhang\t20161231\n",
    " Emily Zhao\t10096273\n",
    " Sam Hossain\t20466500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed engagement data saved to 'output_engagement/processed_engagement_data.xlsx'.\n",
      "Engagement model data saved to 'output_engagement/engagement_model_data.xlsx'.\n",
      "\n",
      "Training Logistic Regression for Engagement...\n",
      "\n",
      "Logistic Regression Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83        32\n",
      "           1       0.84      0.81      0.83        32\n",
      "\n",
      "    accuracy                           0.83        64\n",
      "   macro avg       0.83      0.83      0.83        64\n",
      "weighted avg       0.83      0.83      0.83        64\n",
      "\n",
      "AUC (Logistic Regression): 0.92\n",
      "\n",
      "Training Random Forest for Engagement...\n",
      "\n",
      "Random Forest Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.75      0.84        32\n",
      "           1       0.79      0.97      0.87        32\n",
      "\n",
      "    accuracy                           0.86        64\n",
      "   macro avg       0.88      0.86      0.86        64\n",
      "weighted avg       0.88      0.86      0.86        64\n",
      "\n",
      "AUC (Random Forest): 0.92\n",
      "\n",
      "Training XGBoost for Engagement...\n",
      "\n",
      "XGBoost Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.78      0.83        32\n",
      "           1       0.81      0.91      0.85        32\n",
      "\n",
      "    accuracy                           0.84        64\n",
      "   macro avg       0.85      0.84      0.84        64\n",
      "weighted avg       0.85      0.84      0.84        64\n",
      "\n",
      "AUC (XGBoost): 0.88\n",
      "Engagement predictions saved to 'output_engagement/engagement_predictions.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Constants\n",
    "OUTPUT_DIR = 'output_repayment_risk'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "CURRENT_YEAR = datetime.now().year\n",
    "\n",
    "# Functions\n",
    "\n",
    "def load_data(file_path, sheet_name):\n",
    "    \"\"\"\n",
    "    Load data from an Excel file and strip whitespace from column names.\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df\n",
    "\n",
    "def ensure_column_exists(df, column_name, alternative_names=None):\n",
    "    \"\"\"\n",
    "    Ensure a specific column exists in the DataFrame, possibly under alternative names.\n",
    "    \"\"\"\n",
    "    if column_name in df.columns:\n",
    "        return column_name\n",
    "    if alternative_names:\n",
    "        for alt_name in alternative_names:\n",
    "            if alt_name in df.columns:\n",
    "                df.rename(columns={alt_name: column_name}, inplace=True)\n",
    "                return column_name\n",
    "    raise ValueError(f\"Column '{column_name}' or alternatives {alternative_names} not found.\")\n",
    "\n",
    "def create_financial_features(df):\n",
    "    \"\"\"\n",
    "    Create financial features for analysis.\n",
    "    \"\"\"\n",
    "    # Avoid division by zero\n",
    "    df['Qualified / Verified\\nIncome'].replace(0, np.nan, inplace=True)\n",
    "    df['Loan Term (Months)'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "    # Financial Ratios\n",
    "    df['Debt_to_Income_Ratio'] = df['Outstanding Principal'] / df['Qualified / Verified\\nIncome']\n",
    "    df['Loan_to_Income_Ratio'] = df['Loan Amount'] / df['Qualified / Verified\\nIncome']\n",
    "    df['Monthly_Repayment_Burden'] = df['Outstanding Balance'] / df['Loan Term (Months)']\n",
    "\n",
    "    # Date Calculations\n",
    "    df['Date of Birth'] = pd.to_datetime(df['Date of Birth'], errors='coerce')\n",
    "    df['Age'] = CURRENT_YEAR - df['Date of Birth'].dt.year\n",
    "\n",
    "    df['Disbursement Date'] = pd.to_datetime(df['Disbursement Date'], errors='coerce')\n",
    "    df['Elapsed_Months'] = (datetime.now() - df['Disbursement Date']).dt.days // 30\n",
    "    df['Remaining_Tenure'] = df['Loan Term (Months)'] - df['Elapsed_Months']\n",
    "\n",
    "    # Handle infinite and missing values\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df\n",
    "\n",
    "def validate_features(df):\n",
    "    \"\"\"\n",
    "    Validate engineered features by analyzing correlations and identifying redundant features.\n",
    "    \"\"\"\n",
    "    # Select numeric columns\n",
    "    numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "    # Correlation matrix\n",
    "    correlation_matrix = numeric_df.corr()\n",
    "\n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=False, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "    plt.title(\"Feature Correlation Heatmap\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'repayment_correlation_heatmap.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Identify redundant features\n",
    "    redundant_features = set()\n",
    "    upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "    for column in upper_triangle.columns:\n",
    "        if any(upper_triangle[column].abs() > 0.9):\n",
    "            redundant_features.add(column)\n",
    "\n",
    "    print(f\"Redundant features (correlation > 0.9): {redundant_features}\")\n",
    "    return redundant_features\n",
    "\n",
    "def merge_transunion_data(df, trans_union_path, sheet_name='TransUnion Data'):\n",
    "    \"\"\"\n",
    "    Merge data with TransUnion data.\n",
    "    \"\"\"\n",
    "    trans_union_data = pd.read_excel(trans_union_path, sheet_name=sheet_name)\n",
    "    df['ID'] = df['ID'].astype(str)\n",
    "    trans_union_data['loan_id'] = trans_union_data['loan_id'].astype(str)\n",
    "\n",
    "    merged_data = pd.merge(df, trans_union_data, left_on='ID', right_on='loan_id', how='left')\n",
    "    return merged_data\n",
    "\n",
    "def create_credit_features(df):\n",
    "    \"\"\"\n",
    "    Create additional credit-related features for analysis.\n",
    "    \"\"\"\n",
    "    # Avoid division by zero\n",
    "    df[['revolving_credit_limit', 'instalment_credit_limit']].replace(0, np.nan, inplace=True)\n",
    "\n",
    "    df['Total_Debt'] = df['revolving_credit_balance'] + df['instalment_credit_balance']\n",
    "    df['Total_Credit_Limit'] = df['revolving_credit_limit'] + df['instalment_credit_limit']\n",
    "    df['Debt_to_Credit_Ratio'] = df['Total_Debt'] / df['Total_Credit_Limit']\n",
    "    df['Revolving_Utilization'] = df['revolving_credit_balance'] / df['revolving_credit_limit']\n",
    "    df['Installment_Utilization'] = df['instalment_credit_balance'] / df['instalment_credit_limit']\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df\n",
    "\n",
    "def derive_repayment_risk(df):\n",
    "    \"\"\"\n",
    "    Derive 'Repayment_Risk' feature.\n",
    "    \"\"\"\n",
    "    # Using a threshold for 'Debt_to_Credit_Ratio'\n",
    "    df['Repayment_Risk'] = (df['Debt_to_Credit_Ratio'] > 0.8).astype(int)\n",
    "    return df\n",
    "\n",
    "def validate_credit_features(df):\n",
    "    \"\"\"\n",
    "    Validate credit-related features by analyzing correlations and identifying redundant features.\n",
    "    \"\"\"\n",
    "    # Similar to validate_features, but for credit features\n",
    "    redundant_features = validate_features(df)\n",
    "    return redundant_features\n",
    "\n",
    "def preprocess_data(df, columns_to_remove):\n",
    "    \"\"\"\n",
    "    Preprocess data: remove columns, handle missing values, encode categorical variables, and scale features.\n",
    "    \"\"\"\n",
    "    # Remove unnecessary columns\n",
    "    df.drop(columns=[col for col in columns_to_remove if col in df.columns], inplace=True)\n",
    "\n",
    "    # Drop columns with >50% missing values\n",
    "    df.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)\n",
    "\n",
    "    # Handle missing values\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    if numeric_cols:\n",
    "        imputer_num = SimpleImputer(strategy='median')\n",
    "        df[numeric_cols] = imputer_num.fit_transform(df[numeric_cols])\n",
    "\n",
    "    if categorical_cols:\n",
    "        imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "        df[categorical_cols] = imputer_cat.fit_transform(df[categorical_cols])\n",
    "\n",
    "    # Encode categorical variables\n",
    "    df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Cleans the dataset by replacing infinities and NaN values with the median.\n",
    "    \"\"\"\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    return df.fillna(df.median())\n",
    "\n",
    "def create_model_dataset(df):\n",
    "    \"\"\"\n",
    "    Create dataset for repayment risk prediction.\n",
    "    \"\"\"\n",
    "    repayment_features = [\n",
    "        'Debt_to_Credit_Ratio', 'Revolving_Utilization', 'Installment_Utilization',\n",
    "        'Total_Debt', 'Total_Credit_Limit', 'fico_score', 'count_of_inquiries',\n",
    "        'Debt_to_Income_Ratio', 'Monthly_Repayment_Burden', 'Loan_to_Income_Ratio'\n",
    "    ]\n",
    "    repayment_features = [feat for feat in repayment_features if feat in df.columns]\n",
    "\n",
    "    if 'Repayment_Risk' in df.columns:\n",
    "        # Include 'ID' in the dataset\n",
    "        repayment_data = df[['ID'] + repayment_features + ['Repayment_Risk']]\n",
    "        output_path = os.path.join(OUTPUT_DIR, 'repayment_model_data.xlsx')\n",
    "        repayment_data.to_excel(output_path, index=False)\n",
    "        print(f\"Repayment model data saved to '{output_path}'.\")\n",
    "    else:\n",
    "        print(\"Warning: 'Repayment_Risk' not found. Repayment model data not created.\")\n",
    "\n",
    "def train_repayment_risk_model():\n",
    "    \"\"\"\n",
    "    Train models to predict repayment risk and evaluate their performance.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    data_path = os.path.join(OUTPUT_DIR, 'repayment_model_data.xlsx')\n",
    "    data = pd.read_excel(data_path)\n",
    "\n",
    "    # Define features and target\n",
    "    repayment_features = [\n",
    "        'Debt_to_Credit_Ratio', 'Revolving_Utilization', 'Installment_Utilization',\n",
    "        'Total_Debt', 'Total_Credit_Limit', 'fico_score', 'count_of_inquiries',\n",
    "        'Debt_to_Income_Ratio', 'Monthly_Repayment_Burden', 'Loan_to_Income_Ratio'\n",
    "    ]\n",
    "    repayment_features = [feat for feat in repayment_features if feat in data.columns]\n",
    "    X = data[repayment_features]\n",
    "    y = data['Repayment_Risk']\n",
    "\n",
    "    # Ensure no NaN or infinite values\n",
    "    X = clean_data(X)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Handle class imbalance using SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Train Random Forest\n",
    "    print(\"\\nTraining Random Forest for Repayment Risk...\")\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    rf.fit(X_train_scaled, y_train_resampled)\n",
    "    y_pred_rf = rf.predict(X_test_scaled)\n",
    "    print(\"\\nRandom Forest Report:\")\n",
    "    print(classification_report(y_test, y_pred_rf))\n",
    "    print(f\"AUC (Random Forest): {roc_auc_score(y_test, rf.predict_proba(X_test_scaled)[:, 1]):.2f}\")\n",
    "\n",
    "    # Train XGBoost\n",
    "    print(\"\\nTraining XGBoost for Repayment Risk...\")\n",
    "    xgb = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "    xgb.fit(X_train_resampled, y_train_resampled)\n",
    "    y_pred_xgb = xgb.predict(X_test_scaled)\n",
    "    print(\"\\nXGBoost Report:\")\n",
    "    print(classification_report(y_test, y_pred_xgb))\n",
    "    print(f\"AUC (XGBoost): {roc_auc_score(y_test, xgb.predict_proba(X_test_scaled)[:, 1]):.2f}\")\n",
    "\n",
    "    # Feature importance for Random Forest\n",
    "    feature_importances = pd.Series(rf.feature_importances_, index=repayment_features).sort_values(ascending=False)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_importances.plot(kind='bar', color='skyblue')\n",
    "    plt.title('Feature Importances (Random Forest) - Repayment Risk')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'repayment_feature_importances.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Save predictions for analysis\n",
    "    results = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Random Forest': y_pred_rf,\n",
    "        'XGBoost': y_pred_xgb\n",
    "    })\n",
    "    predictions_output_path = os.path.join(OUTPUT_DIR, 'repayment_predictions.xlsx')\n",
    "    results.to_excel(predictions_output_path, index=False)\n",
    "    print(f\"Repayment risk predictions saved to '{predictions_output_path}'.\")\n",
    "\n",
    "# Main Execution\n",
    "\n",
    "def main():\n",
    "    file_path = 'Data.xlsx'  # Update with your data file path\n",
    "    sheet_name = 'Financial Data'  # Update with your sheet name\n",
    "\n",
    "    # Load data\n",
    "    df = load_data(file_path, sheet_name)\n",
    "\n",
    "    # Ensure necessary columns exist\n",
    "    required_columns = ['ID', 'Outstanding Principal', 'Qualified / Verified\\nIncome', 'Loan Amount', 'Outstanding Balance', 'Loan Term (Months)']\n",
    "    for col in required_columns:\n",
    "        ensure_column_exists(df, col)\n",
    "\n",
    "    # Create financial features\n",
    "    df = create_financial_features(df)\n",
    "\n",
    "    # Validate features and remove redundant ones\n",
    "    redundant_features = validate_features(df)\n",
    "    if redundant_features:\n",
    "        df.drop(columns=redundant_features, inplace=True)\n",
    "        print(\"Dropped redundant features.\")\n",
    "\n",
    "    # Merge with TransUnion data\n",
    "    trans_union_path = 'TransUnion_Data.xlsx'  # Update with your TransUnion data file path\n",
    "    df_merged = merge_transunion_data(df, trans_union_path)\n",
    "\n",
    "    # Create credit features\n",
    "    df_merged = create_credit_features(df_merged)\n",
    "\n",
    "    # Validate credit features and remove redundant ones\n",
    "    redundant_features = validate_credit_features(df_merged)\n",
    "    if redundant_features:\n",
    "        df_merged.drop(columns=redundant_features, inplace=True)\n",
    "        print(\"Dropped redundant credit features.\")\n",
    "\n",
    "    # Derive repayment risk\n",
    "    df_merged = derive_repayment_risk(df_merged)\n",
    "\n",
    "    # Preprocess data\n",
    "    columns_to_remove = [\n",
    "        'ID', 'Gender', 'Date of Birth', 'Disbursement Date', 'loan_id',\n",
    "        'Loan Amount', 'Outstanding Balance', 'Outstanding Principal',\n",
    "        'Loan Term (Months)', 'Stated Income on application',\n",
    "        'Qualified / Verified\\nIncome', 'Revolving_Utilization', 'Installment_Utilization'\n",
    "    ]\n",
    "    df_encoded = preprocess_data(df_merged.copy(), columns_to_remove)\n",
    "\n",
    "    # Save processed data\n",
    "    processed_output_path = os.path.join(OUTPUT_DIR, 'processed_repayment_data.xlsx')\n",
    "    df_encoded.to_excel(processed_output_path, index=False)\n",
    "    print(f\"Processed repayment risk data saved to '{processed_output_path}'.\")\n",
    "\n",
    "    # Create dataset for model\n",
    "    create_model_dataset(df_encoded)\n",
    "\n",
    "    # Train and evaluate the repayment risk model\n",
    "    train_repayment_risk_model()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
