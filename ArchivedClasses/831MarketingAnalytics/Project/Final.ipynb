{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team Gordon\n",
    "\n",
    "Student Name\tStudent Number\n",
    " Alisha Sahota\t20497348\n",
    " Anthony Ramelo\t20499391\n",
    " Chris Wu\t10182394\n",
    " Elizabeth Zhang\t20161231\n",
    " Emily Zhao\t10096273\n",
    " Sam Hossain\t20466500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redundant features (correlation > 0.9): {'Total Activities', 'Average activities per day', 'Engagement_Regularity', 'Outstanding Principal', 'Quiz Count', 'Inspiration Count', 'Engagement_to_Income_Ratio', 'Elapsed_Months'}\n",
      "Dropped redundant features.\n",
      "Processed engagement data saved to 'output_engagement/processed_engagement_data.xlsx'.\n",
      "Engagement model data saved to 'output_engagement/engagement_model_data.xlsx'.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Engagement_Regularity', 'Total Activities', 'Engagement_to_Income_Ratio'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 302\u001b[0m\n\u001b[1;32m    299\u001b[0m     train_engagement_model()\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 302\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[26], line 299\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m create_model_dataset(df)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# Train and evaluate the engagement model\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m train_engagement_model()\n",
      "Cell \u001b[0;32mIn[26], line 192\u001b[0m, in \u001b[0;36mtrain_engagement_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Define features and target\u001b[39;00m\n\u001b[1;32m    183\u001b[0m engagement_features \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMood Count\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConsistency_Score\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEngagement_to_Income_Ratio\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    191\u001b[0m ]\n\u001b[0;32m--> 192\u001b[0m X \u001b[38;5;241m=\u001b[39m data[engagement_features]\n\u001b[1;32m    193\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEngagement_Level\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Split the data\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6179\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Engagement_Regularity', 'Total Activities', 'Engagement_to_Income_Ratio'] not in index\""
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Constants\n",
    "OUTPUT_DIR = 'output_engagement'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "CURRENT_YEAR = datetime.now().year\n",
    "\n",
    "# Functions\n",
    "\n",
    "def load_data(file_path, sheet_name):\n",
    "    \"\"\"\n",
    "    Load data from an Excel file and strip whitespace from column names.\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df\n",
    "\n",
    "def ensure_column_exists(df, column_name, alternative_names=None):\n",
    "    \"\"\"\n",
    "    Ensure a specific column exists in the DataFrame, possibly under alternative names.\n",
    "    \"\"\"\n",
    "    if column_name in df.columns:\n",
    "        return column_name\n",
    "    if alternative_names:\n",
    "        for alt_name in alternative_names:\n",
    "            if alt_name in df.columns:\n",
    "                df.rename(columns={alt_name: column_name}, inplace=True)\n",
    "                return column_name\n",
    "    raise ValueError(f\"Column '{column_name}' or alternatives {alternative_names} not found.\")\n",
    "\n",
    "def create_engagement_features(df):\n",
    "    \"\"\"\n",
    "    Create engagement-related features for analysis.\n",
    "    \"\"\"\n",
    "    # Avoid division by zero\n",
    "    df['Average total activities per month'].replace(0, np.nan, inplace=True)\n",
    "    df['Average activities per day'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "    # Engagement Metrics\n",
    "    df['Consistency_Score'] = df['Average activities per day'] / df['Average total activities per month']\n",
    "    df['Engagement_Regularity'] = df['Consistency_Score'] * df['Average activities per day']\n",
    "\n",
    "    # Date Calculations\n",
    "    df['Disbursement Date'] = pd.to_datetime(df['Disbursement Date'], errors='coerce')\n",
    "    df['Elapsed_Months'] = (datetime.now() - df['Disbursement Date']).dt.days // 30\n",
    "\n",
    "    df['Missed_Periods'] = df['Elapsed_Months'] - (df['Total Activities'] / df['Average total activities per month'])\n",
    "    df['Activity_Diversity'] = df[['Quiz Count', 'Mood Count', 'Inspiration Count']].gt(0).sum(axis=1)\n",
    "    df['Engagement_to_Income_Ratio'] = df['Total Activities'] / df['Qualified / Verified\\nIncome']\n",
    "\n",
    "    # Handle infinite and missing values\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df\n",
    "\n",
    "def validate_features(df):\n",
    "    \"\"\"\n",
    "    Validate engineered features by analyzing correlations and identifying redundant features.\n",
    "    \"\"\"\n",
    "    # Select numeric columns\n",
    "    numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "    # Correlation matrix\n",
    "    correlation_matrix = numeric_df.corr()\n",
    "\n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=False, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "    plt.title(\"Feature Correlation Heatmap\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'engagement_correlation_heatmap.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Identify redundant features\n",
    "    redundant_features = set()\n",
    "    upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "    for column in upper_triangle.columns:\n",
    "        if any(upper_triangle[column].abs() > 0.9):\n",
    "            redundant_features.add(column)\n",
    "\n",
    "    print(f\"Redundant features (correlation > 0.9): {redundant_features}\")\n",
    "    return redundant_features\n",
    "\n",
    "def preprocess_data(df, columns_to_remove):\n",
    "    \"\"\"\n",
    "    Preprocess data: remove columns, handle missing values, encode categorical variables, and scale features.\n",
    "    \"\"\"\n",
    "    # Remove unnecessary columns\n",
    "    df.drop(columns=[col for col in columns_to_remove if col in df.columns], inplace=True)\n",
    "\n",
    "    # Drop columns with >50% missing values\n",
    "    df.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)\n",
    "\n",
    "    # Handle missing values\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    if numeric_cols:\n",
    "        imputer_num = SimpleImputer(strategy='median')\n",
    "        df[numeric_cols] = imputer_num.fit_transform(df[numeric_cols])\n",
    "\n",
    "    if categorical_cols:\n",
    "        imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "        df[categorical_cols] = imputer_cat.fit_transform(df[categorical_cols])\n",
    "\n",
    "    # Encode categorical variables\n",
    "    df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "    return df\n",
    "\n",
    "def derive_engagement_level(df):\n",
    "    \"\"\"\n",
    "    Derive 'Engagement_Level' feature.\n",
    "    \"\"\"\n",
    "    if 'Consistency_Score' in df.columns:\n",
    "        median_consistency = df['Consistency_Score'].median()\n",
    "        df['Engagement_Level'] = (df['Consistency_Score'] >= median_consistency).astype(int)\n",
    "    else:\n",
    "        print(\"Warning: 'Consistency_Score' not found. Cannot derive 'Engagement_Level'.\")\n",
    "    return df\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Cleans the dataset by replacing infinities and NaN values with the median.\n",
    "    \"\"\"\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    return df.fillna(df.median())\n",
    "\n",
    "def create_model_dataset(df):\n",
    "    \"\"\"\n",
    "    Create dataset for engagement prediction.\n",
    "    \"\"\"\n",
    "    # Engagement features\n",
    "    engagement_features = [\n",
    "        'Mood Count',\n",
    "        'Consistency_Score',\n",
    "        'Engagement_Regularity',\n",
    "        'Activity_Diversity',\n",
    "        'Total Activities',\n",
    "        'Missed_Periods',\n",
    "        'Engagement_to_Income_Ratio'\n",
    "    ]\n",
    "    engagement_features = [feat for feat in engagement_features if feat in df.columns]\n",
    "\n",
    "    if 'Engagement_Level' in df.columns:\n",
    "        # Include 'ID' in the dataset\n",
    "        engagement_data = df[['ID'] + engagement_features + ['Engagement_Level']]\n",
    "        output_path = os.path.join(OUTPUT_DIR, 'engagement_model_data.xlsx')\n",
    "        engagement_data.to_excel(output_path, index=False)\n",
    "        print(f\"Engagement model data saved to '{output_path}'.\")\n",
    "    else:\n",
    "        print(\"Warning: 'Engagement_Level' not found. Engagement model data not created.\")\n",
    "\n",
    "def train_engagement_model():\n",
    "    \"\"\"\n",
    "    Train models to predict engagement levels and evaluate their performance.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    data_path = os.path.join(OUTPUT_DIR, 'engagement_model_data.xlsx')\n",
    "    data = pd.read_excel(data_path)\n",
    "\n",
    "    # Define features and target\n",
    "    engagement_features = [\n",
    "        'Mood Count',\n",
    "        'Consistency_Score',\n",
    "        'Engagement_Regularity',\n",
    "        'Activity_Diversity',\n",
    "        'Total Activities',\n",
    "        'Missed_Periods',\n",
    "        'Engagement_to_Income_Ratio'\n",
    "    ]\n",
    "    X = data[engagement_features]\n",
    "    y = data['Engagement_Level']\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Clean data\n",
    "    X_train = clean_data(X_train)\n",
    "    X_test = clean_data(X_test)\n",
    "\n",
    "    # Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Train Logistic Regression\n",
    "    print(\"\\nTraining Logistic Regression for Engagement...\")\n",
    "    lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "    y_pred_lr = lr.predict(X_test_scaled)\n",
    "    print(\"\\nLogistic Regression Report:\")\n",
    "    print(classification_report(y_test, y_pred_lr))\n",
    "    print(f\"AUC (Logistic Regression): {roc_auc_score(y_test, lr.predict_proba(X_test_scaled)[:, 1]):.2f}\")\n",
    "\n",
    "    # Train Random Forest\n",
    "    print(\"\\nTraining Random Forest for Engagement...\")\n",
    "    rf = RandomForestClassifier(max_depth=5, n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    y_pred_rf = rf.predict(X_test_scaled)\n",
    "    print(\"\\nRandom Forest Report:\")\n",
    "    print(classification_report(y_test, y_pred_rf))\n",
    "    print(f\"AUC (Random Forest): {roc_auc_score(y_test, rf.predict_proba(X_test_scaled)[:, 1]):.2f}\")\n",
    "\n",
    "    # Train XGBoost\n",
    "    print(\"\\nTraining XGBoost for Engagement...\")\n",
    "    xgb = XGBClassifier(learning_rate=0.01, max_depth=3, n_estimators=100, random_state=42, eval_metric='logloss')\n",
    "    xgb.fit(X_train_scaled, y_train)\n",
    "    y_pred_xgb = xgb.predict(X_test_scaled)\n",
    "    print(\"\\nXGBoost Report:\")\n",
    "    print(classification_report(y_test, y_pred_xgb))\n",
    "    print(f\"AUC (XGBoost): {roc_auc_score(y_test, xgb.predict_proba(X_test_scaled)[:, 1]):.2f}\")\n",
    "\n",
    "    # Feature importance for Random Forest\n",
    "    feature_importances = pd.Series(rf.feature_importances_, index=engagement_features).sort_values(ascending=False)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_importances.plot(kind='bar', color='skyblue')\n",
    "    plt.title('Feature Importances (Random Forest) - Engagement')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'engagement_feature_importances.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Save predictions for analysis\n",
    "    results = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Logistic Regression': y_pred_lr,\n",
    "        'Random Forest': y_pred_rf,\n",
    "        'XGBoost': y_pred_xgb\n",
    "    })\n",
    "    predictions_output_path = os.path.join(OUTPUT_DIR, 'engagement_predictions.xlsx')\n",
    "    results.to_excel(predictions_output_path, index=False)\n",
    "    print(f\"Engagement predictions saved to '{predictions_output_path}'.\")\n",
    "\n",
    "# Main Execution\n",
    "\n",
    "def main():\n",
    "    file_path = 'Data 3 - October, 2024.xlsx'  # Update with your data file path\n",
    "    sheet_name = 'Parachute - Cross Section'  # Update with your sheet name\n",
    "\n",
    "    # Load data\n",
    "    df = load_data(file_path, sheet_name)\n",
    "\n",
    "    # Ensure 'Total Activities' column exists\n",
    "    total_activities_col = ensure_column_exists(df, 'Total Activities', alternative_names=['Total Activities '])\n",
    "\n",
    "    # Create engagement features\n",
    "    df = create_engagement_features(df)\n",
    "\n",
    "    # Validate features and remove redundant ones\n",
    "    redundant_features = validate_features(df)\n",
    "    if redundant_features:\n",
    "        df.drop(columns=redundant_features, inplace=True)\n",
    "        print(\"Dropped redundant features.\")\n",
    "\n",
    "    # Preprocess data\n",
    "    columns_to_remove = [\n",
    "        'ID', 'Gender', 'Date of Birth', 'Disbursement Date',\n",
    "        'Loan Amount', 'Outstanding Balance', 'Outstanding Principal',\n",
    "        'Loan Term (Months)', 'Stated Income on application',\n",
    "        'Qualified / Verified\\nIncome'\n",
    "    ]\n",
    "    df_encoded = preprocess_data(df.copy(), columns_to_remove)\n",
    "\n",
    "    # Derive engagement level\n",
    "    df = derive_engagement_level(df)\n",
    "\n",
    "    # Save processed data\n",
    "    processed_output_path = os.path.join(OUTPUT_DIR, 'processed_engagement_data.xlsx')\n",
    "    df.to_excel(processed_output_path, index=False)\n",
    "    print(f\"Processed engagement data saved to '{processed_output_path}'.\")\n",
    "\n",
    "    # Create dataset for model\n",
    "    create_model_dataset(df)\n",
    "\n",
    "    # Train and evaluate the engagement model\n",
    "    train_engagement_model()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
